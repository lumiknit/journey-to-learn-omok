{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11_rl_notes.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RL Notes\n",
        "\n",
        "이론 정리용"
      ],
      "metadata": {
        "id": "MFbSO0llZUoO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Markov\n",
        "\n",
        "- Markov Property는 간단히 말해서 '어떤 stochastic process $X$가 있을 때, $X_1, \\cdots, X_{t-1}$로\n",
        "결정되는 $X_t$의 확률이 $X_{t - 1}$만으로 결정이 된다'는 것.\n",
        "- 이를 만족하는 stochastic process를 Markov\n",
        "process라고 부름.\n",
        "- Markovian은 memoryless라고 볼 수 있음.\n",
        "- Markov Decision Process는 Markov model\n",
        "중 하나로, state set $S$, actions set $A$,\n",
        "state transition matrix set $P$, reward $R$,\n",
        "discound factor $\\gamma$로 이루어진\n",
        "Markov model의 일종임.\n",
        "- Set of state $S$는 가능한 모든 상태의 집합,\n",
        "set of action $A$는 가능한 모든 행동의 집합임.\n",
        "- State transition matrix $P$의 원소는\n",
        "특정 상태에서 특정 행동을 했을 때 특정 상태로 전이될\n",
        "확률을 나타냄.\n",
        "예를 들어서 $P_{s, s', a}$처럼 나타낸다고 하자.\n",
        "- Reward function $R$은 특정 상태에서 특정\n",
        "행동을 할 경우에 얻는 보상에 대한 함수임.\n",
        "- Discount factor $\\gamma \\in [0, 1]$는 미래의 보상이\n",
        "현재 시점에서 얼마나 약하게 작용하는지 나타내는 매개변수임. 0에 가까우면 당장의 보상이 중요하며,\n",
        "1에 가까울수록 미래의 보상의 비중이 커짐.\n",
        "- Episode는 위 같은 정보들이 있을 때, 처음 상태에서\n",
        "출발해서 특정 전략에 따라 행동을 선택해가며 보상을\n",
        "획득하는 일련의 과정을 말함.\n",
        "- Return $G_t$는 $t$ 시점 기준에서\n",
        "이후에 얻을 보상들에 discunt factor를 적용한 합임.\n",
        "즉, $G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$"
      ],
      "metadata": {
        "id": "VKb2i_4hZf98"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy and Value Function\n",
        "\n",
        "- Policy function $\\pi(a | s)$는 어떤 상태 $s$\n",
        "에 있을 때 행동 $a$를 할 확률을 나타냄.\n",
        "아래에서는 policy가 parameter로 표현되는 경우가 많아\n",
        "$\\pi_\\theta$와 같이 씀.\n",
        "- Value function은 특정 상태나 행동에 가치를\n",
        "매기는 함수를 말함.\n",
        "- State value function $v(s) = E[G_t \\mid S_t=s]$는\n",
        "특정 상태에서 얻을 수 있는 기댓값에 대한 함수.\n",
        "- Action value function\n",
        "$q(s, a) = E[G_t \\mid S_t=s, A_t=a]$\n",
        "는 특정 상태에서 특정 행동을 했을 때 얻을 수 있는\n",
        "보상의 기댓값에 대한 함수.\n",
        "- 이 떄 특정 policy $\\pi$에 대한 value를 생각할\n",
        "수 있는데, 이를 $v_\\pi$나 $q_\\pi$와 같이 나타냄.\n",
        "- 정의에 따르면\n",
        "$v_\\pi(s) = E_\\pi[R_t + \\gamma v_\\pi(S_{t+1}) \\mid S_t = s]$,\n",
        "$q_\\pi(s, a) = E_\\pi[R_t + \\gamma q_\\pi(S_{t+1}, A_{t+1}) \\mid S_t = s, A_t = a]$\n",
        "이라고 할 수 있음.\n",
        "- Policy는 어떤 state가 주어졌을 때 특정 행동을\n",
        "할 확률이며, $v$는 이 확률에 따라 행동할 때 얻을\n",
        "수 있는 보상 $q$의 기댓값이므로,\n",
        "$v_\\pi(s) = \\sum_{a \\in A} \\pi(a \\mid s) q(s, a)$\n",
        "- 반대로 특정 상태에서 특정 행동을 했을 때\n",
        "얻는 보상은 $R$\n",
        "다른 상태로 전이될 확률은 $P$로 나타나고, \n",
        "그 상태에서의 value를 $v$로 나타내므로\n",
        "$q_\\pi(s, a) = R(s, a) + \\gamma \\sum_{s' \\in S} P_{s, s', a} v(s')$\n",
        "- 위 두 식을 이용하면 $v$나 $q$를 다른 하나를\n",
        "쓰지 않고 기술할 수 있음.\n",
        "- Value function이 optimial하다는 것은,\n",
        "모든 policy 중에서 해당 형태의 함수를 최대로\n",
        "만드는 policy $\\pi^*$에 대한 value function을\n",
        "말함.\n",
        "- 반대로 optimal policy는 최종적으로 얻는 보상을\n",
        "최대로 하는 함수이며, 이 때문에\n",
        "$\\pi^*(a \\mid s) = \\mathbb{1}_{a = \\text{argmax}_{a \\in A}} q^*(s, a)$\n",
        "처럼 적을 수 있다.\n",
        "- MDP에서 강화학습을 한다는 것은\n",
        "$\\pi^*$를 찾거나 이를 근사하는 것이라고 할 수 있고,\n",
        "이를 찾기 위해서 $v^*$나 $q^*$를 찾는 것을\n",
        "목표로 할 수 있음.\n",
        "- 만약 $S, A$가 연속적인 공간이면\n",
        "$v_\\pi(s) = \\int_{A} \\pi(a \\mid s) q(s, a) da$\n",
        "$q_\\pi(s, a) = R(s, a) + \\gamma \\int_{S} P_{s,s',a} v(s') ds'$\n",
        "처럼 적분꼴로 바꿔서 생각하면 됨."
      ],
      "metadata": {
        "id": "yCYviCDoISPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dynamic Programming and MC/TD Learning\n",
        "\n",
        "- Policy iteration은 임의의 policy $pi$에 대해\n",
        "Bellman equation을 모든 state와 action에 적용해서\n",
        "새로운 value function을 얻는 *policy evalution*\n",
        "을 한 뒤, 이 떄의 value function의 최댓값을 선택하도록\n",
        "하는 policy $\\pi'$을 선택하는 policy improvement\n",
        "를 반복함.\n",
        "- Value iteration은 '결국에 optimal policy는 value를 최대로 하는 정책이다'라는 정의를 이용해서,\n",
        "value를 업데이트 할 때 각 action에 대해 $\\pi$\n",
        "를 사용하는 대신, 바로 최댓값을 사용하도록 함.\n",
        "즉, $v(s) \\leftarrow \\sum_{a \\in A} \\pi(a \\mid s) q(s, a)$ 대신 $v(s) \\leftarrow \\max_{a \\in A} q(s, a)$를 적용함.\n",
        "- 일반적으로 policy iteration이 value iteration에 비해\n",
        "빠르고 좋게 수렴함.\n",
        "(Value iteration은 value function 자체를\n",
        "policy로 하여 업데이트를 하기 때문에,\n",
        "policy를 충분히 평가하기 전에 update하는 꼴이 되지만,\n",
        "policy iteration은 policy를 충분히 평가한 뒤에\n",
        "한번에 갱신하기 때문에 안정적이고 빠르다고 함.)\n",
        "- Monte Carlo Learning은 policy evaluation\n",
        "대신에 Monte Carlo evaluation을 하는데,\n",
        "방법은 임의의 episode에서 $s \\in S$에서\n",
        "return $G_t$를 얻었다면 이 $G_t$\n",
        "를 누적하는\n",
        "$S(s)$, 해당 state를 방문한 횟수인 $N(s)$를\n",
        "갱신하고, $v_\\pi(s) \\simeq S(s) / N(s)$\n",
        "라고 근사하여 사용.\n",
        "- 만약 $s \\in S$를 $N$번째 탐색해서 총합 $S$를\n",
        "얻은 상태에서 새로운 return $G$가 발생했다면\n",
        "$S' \\leftarrow \\frac{NS + G}{N + 1}\n",
        "= S + \\frac{1}{N + 1}(G - S)$\n",
        "가 되는데, 이 횟수를 저장하지 않고 적당한 매개변수\n",
        "$\\alpha \\in (0, 1)$로 근사해서\n",
        "$S' \\leftarrow S + \\alpha(G - S)$\n",
        "처럼 exponential moving average를 사용할 수 있음.\n",
        "\n",
        "- 위에서 $G_t$는 episode가 완전히 끝나야\n",
        "계산하는 것이 가능한데, 이전의 관계식\n",
        "$G_t = R_{t+1} + \\gamma v(S_{t+1})$ (TD target) 등으로 근사해서 사용할 수 있음.\n",
        "이 evaluation을 사용하는 것이 TD(0) Learning임.\n",
        "- 여기서 $v$ 대신 $q$를 사용하는 evaluation이 Sarsa임.\n",
        "- 여기서 $G_t$ 대신에 특정 $n$번째 discounted\n",
        "reward subseries $G_t^n$을 사용한 것이\n",
        "$n$-step prediction임.\n",
        "TD(0)는 $1$-step이며, Monte Carlo는 $\\infty$-step임.\n",
        "- 여기서 $G_t^n$ 대신에\n",
        "$G_t^\\lambda = (1 - \\lambda) \\sum_{n=1}^{\\infty}\n",
        "\\lambda^{n - 1} G_t^n$을 사용하는 것이\n",
        "TD($\\lambda$)임.\n",
        "- TD(\\lambda)와 비슷하게 Sarsa에도 weighted sum\n",
        "을 적용해서 Sarsa($\\lambda$)를 만들 수 있음.\n",
        "- 이 $\\lambda$-step의 경우에는\n",
        "이후에 있을 가중치를 게산하여 사용할 수도 있고 (forward)\n",
        "반대로 지금까지 있었던 episode를 되짚어가며\n",
        "이미 지나온 상태를 update할 수도 있음 (backward).\n",
        "- 위에서는 다음 상태로의 행동을 정하는 behaviour\n",
        "policy와 여기서 값을 update할 때 사용하는\n",
        "policy인 alternative policy를 같은 것을 사용하지만,\n",
        "이 둘이 같을 필요는 없으며, 그런 것을\n",
        "off-policy learning이라고 함.\n",
        "- 기존에는 특정 episode만 반복하는 것을 피하기 위해\n",
        "$\\epsilon$의 확률로 임의의 행동을 하는\n",
        "$\\epsilon$-greedy를 policy로 하는데,\n",
        "behaviour만 $\\epsilon$-greedy를 사용하고\n",
        "alternative로 그냥 greedy를 사용하는 것이\n",
        "Q-learning임.\n",
        "- Q-learning의 경우, 탐색한 뒤에\n",
        "update를 위한 값을 greedy하게 고르므로, 현재\n",
        "evaluation중인 $\\pi$ 대신에, 그냥\n",
        "다음 상태 $s'$에 대해 $q$를 최대로 하는 행동 $a$\n",
        "를 적용하면 됨.\n",
        "즉, 위의 $G_t$가 $R + \\gamma \\max_{a \\in A} q(s', a)$로 바뀜.\n",
        "\n",
        "- DP에서 탐색 공간이 너무 넓어서 sampling을 사용한 MC로 전환, episode가 끝나지 않는 경우에 MC를\n",
        "쓰기 힘들어 TD(0)/Sarsa를 사용,\n",
        "TD/MC의 절충안으로 $n$-step, $\\lambda$를 사용,\n",
        "탐색과 갱신이 같은 policy여서 optimal을 잘\n",
        "찾지 못하는 문제 때문에 Q-Learning등\n",
        "off-policy를 사용\n",
        "\n",
        "- DQN에서는 여기서 나온 함수들을 neural network와\n",
        "그를 적용한 함수들로 나타내고, 이를 GD로 근사하게\n",
        "되는데,\n",
        "이 때 agent가 진행하는 episode의 state, next state,\n",
        "action, reward를 replay memory에 저장한 뒤에\n",
        "sampling하여 학습시킴."
      ],
      "metadata": {
        "id": "UouPvzHNN08s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy Gradient\n",
        "\n",
        "- Policy iteration은 policy를 value function에\n",
        "대해 greedy하게 만들다보니, value function에 따라\n",
        "급격하게 바뀌고, optimal한 결과만 추려냄.\n",
        "- 이 대신에 policy를 직접 근사하기 위해서\n",
        "policy에 대한 loss를 부여하고 이를 gradient를 통해\n",
        "optimal policy로 수렴시키는 것이 policy gradient임.\n",
        "- 여기서는 policy를 $\\theta$에 대해 parametrize하여\n",
        "$\\pi_\\theta$처럼 나타냄.\n",
        "- Optimal policy는 결국 reward를 최대화 하는\n",
        "policy다보니, 이를 평가하기 위한 objective function\n",
        "$J$ 역시 reward에 대한 기댓값이 되어야 함.\n",
        "- 이 때문에, 만약 모든 episode가 유한한 시점에서\n",
        "종료되는 경우에는 최초의 상태 $s_0 \\in S$에서\n",
        "얻을 수 있는 reward의 기댓값이 곧 평가치임.\n",
        "즉, $J(\\theta) = v_{\\pi_\\theta}(s_0)$\n",
        "- 만약 그렇지 않은 경우에는, stationary distribution (transition matrix $P$에 대해\n",
        "$aP = a$를 만족하는 $a$.\n",
        "다르게 보자면 state transtion에 대해 불변하는\n",
        "임의의 vector $a$, 여기서는 특정 상태가\n",
        "차지하는 가중치의 일종) $d$에 대해,\n",
        "$J(\\theta) = \\sum_{s \\in S} d_{\\pi_\\theta}(s) v_{\\pi_\\theta}(s)$\n",
        "같이 가능한 모든 상태에 대한 value function의 합이나\n",
        "$J(\\theta) = \\sum_{s \\in S} d_{\\pi_\\theta}(s) \\sum_{a \\in A} \\pi_\\theta(s, a) R(s, a)$\n",
        "와 같이 transtition에서 얻는 reward의 합으로\n",
        "평가할 수 있다.\n",
        "- 이 둘중 첫번째 (start value, $J_1$)와 세번째 (average reward per time-step, $J_{\\text{av}R}$ 이 많이 쓰인다고.\n",
        "- 이제 이 $J$를 미분가능하다고 가정하든 뭐든 방법을 써서\n",
        "$J$의 극대점, 이상적으로는 최대점 $\\theta$를 찾는 것이\n",
        "목표이며, $\\delta J$를 이용해서\n",
        "$\\theta \\leftarrow \\theta + \\alpha \\delta J$처럼\n",
        "gradient ascent를 하는 것이 policy gradient.\n",
        "- Finte difference policy gradient는\n",
        "각 parameter를 각 방향으로 작은 변화량을 줘서 gradient를 계산.\n",
        "미분가능성과 무관하고 쉽지만 수렴이 느림.\n",
        "- $J = J_{\\text{av}R}$가 미분 가능하다고 가정하고\n",
        "gradient를 취하면\n",
        "$\\nabla f = f \\frac{\\nabla f}{f} = f \\nabla \\log f$에 따라\n",
        "$$\\nabla_\\theta J(\\theta) = \\sum_s d(s) \\sum_a \\pi_\\theta(s, a) \\nabla_\\theta \\log \\pi_\\theta(s, a) R(s, a) = E[\\nabla_\\theta \\log \\pi_\\theta(s, a) R] $$\n",
        "와 같이 정리됨. (이 중 $\\nabla \\log \\pi$가 score function이 됨.)\n",
        "- 즉,\n",
        "$\\theta \\leftarrow \\theta + \\alpha E[\\nabla_\\theta \\log \\pi_\\theta(s,a) r]$\n",
        "와 같이 update하도록 생각해볼 수 있음.\n",
        "- 이 때 reward $r$을 $q_{\\pi_\\theta}(s, a)$로 바꿔도 같은 값을 가지며,\n",
        "$q_{\\pi_\\theta}(s, a)$는 Monte-Carlo\n",
        "policy evaluation에서 각 episode가 끝날 때\n",
        "해당 episode를 backtrack하며 reward를 누적하여서\n",
        "근사할 수 있음.\n",
        "이를 이용해\n",
        "모든 상태와 행동에 대해 expectation을 계산하는 대신에\n",
        "episode를 반복하면서 해당 episode 중의 상태와 행동에\n",
        "대해 획득한 보상 $v_t$를 $r$ 대신 사용하여\n",
        "$theta$를 갱신하는 것이\n",
        "Monte-Carlo Policy Gradient, 또는 REINFORCE 알고리즘임.\n",
        "- Monte Carlo와 같이 episode가 빨리 끝나는 경우가\n",
        "아니면 $q$를 그냥 사용해야함.\n",
        "문제는 $q$를 모르기 때문에 이 역시 근사를 해야되며,\n",
        "이전의 iteration/evaluation 등으로 $q$를 학습하고\n",
        "그 $q$를 바탕으로 policy $\\pi$를 근사하는 것을\n",
        "반복해야함.\n",
        "이 때 policy $\\pi$가 actor (critic이 지시한 대로\n",
        "행동), $q$가 critic (actor를 바탕으로 탐색하여\n",
        "actor를 평가하도록 함)라고 부름.\n",
        "- 학습의 variance를 줄이기 위해,\n",
        "각 행동으로 얻을 수 있는 보상을 $v$로\n",
        "보정한 $a = q - v$를 학습에 사용하는 것이\n",
        "baseline."
      ],
      "metadata": {
        "id": "o9omIwiuYDMJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reference\n",
        "\n",
        "- [Fundamental of Reinforcement Learning, Woong won, Lee](https://dnddnjs.gitbooks.io/rl/content/)"
      ],
      "metadata": {
        "id": "7psHyrXwh-50"
      }
    }
  ]
}