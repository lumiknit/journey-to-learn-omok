{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "14_reinforce_and_tree_search_on_connect_4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 14. REINFORCE and Tree Search on Connect 4\n"
      ],
      "metadata": {
        "id": "EUsgzpHxQbS1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 지금까지 해온 것들을 모두 모으자!\n",
        "\n",
        "여기서는 policy gradient (REINFORCE),\n",
        "tree search with policy를 혼용해서\n",
        "`agent_greedy`를 뛰어넘는\n",
        "Connect 4 policy를 강화학습 시켜봅니다."
      ],
      "metadata": {
        "id": "EoHuV_ASQhv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "방식은 다음과 같습니다.\n",
        "\n",
        "- Episode에 대해서는 Monte Carlo policy gradient를\n",
        "사용하여 policy를 갱신.\n",
        "- Policy는 policy network를 base policy로 하여\n",
        "tree search with policy를 사용.\n",
        "\n",
        "학습을 할 때 누구와 싸워가며 학습할지를 생각해볼 수 있는데,\n",
        "이 부분은 조금 뒤에서 다시 얘기하겠습니다."
      ],
      "metadata": {
        "id": "iM_iMsn4Q6Vl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 구현"
      ],
      "metadata": {
        "id": "cbMXrLnS2TtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf mock4.py m4\n",
        "!git clone https://github.com/lumiknit/mock4.py.git\n",
        "!mv mock4.py m4\n",
        "!mv m4/mock4.py .\n",
        "import mock4\n",
        "from mock4 import Mock4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5qWD4hr2WMH",
        "outputId": "77ef685b-a7da-4b3c-99fc-f9526806f401"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mock4.py'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Counting objects:   3% (1/28)\u001b[K\rremote: Counting objects:   7% (2/28)\u001b[K\rremote: Counting objects:  10% (3/28)\u001b[K\rremote: Counting objects:  14% (4/28)\u001b[K\rremote: Counting objects:  17% (5/28)\u001b[K\rremote: Counting objects:  21% (6/28)\u001b[K\rremote: Counting objects:  25% (7/28)\u001b[K\rremote: Counting objects:  28% (8/28)\u001b[K\rremote: Counting objects:  32% (9/28)\u001b[K\rremote: Counting objects:  35% (10/28)\u001b[K\rremote: Counting objects:  39% (11/28)\u001b[K\rremote: Counting objects:  42% (12/28)\u001b[K\rremote: Counting objects:  46% (13/28)\u001b[K\rremote: Counting objects:  50% (14/28)\u001b[K\rremote: Counting objects:  53% (15/28)\u001b[K\rremote: Counting objects:  57% (16/28)\u001b[K\rremote: Counting objects:  60% (17/28)\u001b[K\rremote: Counting objects:  64% (18/28)\u001b[K\rremote: Counting objects:  67% (19/28)\u001b[K\rremote: Counting objects:  71% (20/28)\u001b[K\rremote: Counting objects:  75% (21/28)\u001b[K\rremote: Counting objects:  78% (22/28)\u001b[K\rremote: Counting objects:  82% (23/28)\u001b[K\rremote: Counting objects:  85% (24/28)\u001b[K\rremote: Counting objects:  89% (25/28)\u001b[K\rremote: Counting objects:  92% (26/28)\u001b[K\rremote: Counting objects:  96% (27/28)\u001b[K\rremote: Counting objects: 100% (28/28)\u001b[K\rremote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects:   4% (1/23)\u001b[K\rremote: Compressing objects:   8% (2/23)\u001b[K\rremote: Compressing objects:  13% (3/23)\u001b[K\rremote: Compressing objects:  17% (4/23)\u001b[K\rremote: Compressing objects:  21% (5/23)\u001b[K\rremote: Compressing objects:  26% (6/23)\u001b[K\rremote: Compressing objects:  30% (7/23)\u001b[K\rremote: Compressing objects:  34% (8/23)\u001b[K\rremote: Compressing objects:  39% (9/23)\u001b[K\rremote: Compressing objects:  43% (10/23)\u001b[K\rremote: Compressing objects:  47% (11/23)\u001b[K\rremote: Compressing objects:  52% (12/23)\u001b[K\rremote: Compressing objects:  56% (13/23)\u001b[K\rremote: Compressing objects:  60% (14/23)\u001b[K\rremote: Compressing objects:  65% (15/23)\u001b[K\rremote: Compressing objects:  69% (16/23)\u001b[K\rremote: Compressing objects:  73% (17/23)\u001b[K\rremote: Compressing objects:  78% (18/23)\u001b[K\rremote: Compressing objects:  82% (19/23)\u001b[K\rremote: Compressing objects:  86% (20/23)\u001b[K\rremote: Compressing objects:  91% (21/23)\u001b[K\rremote: Compressing objects:  95% (22/23)\u001b[K\rremote: Compressing objects: 100% (23/23)\u001b[K\rremote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 28 (delta 8), reused 15 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (28/28), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: {}\".format(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ujYDo5o2W2J",
        "outputId": "bc58ae58-21c0-42c7-8289-42eddcd62fff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Scope: pass"
      ],
      "metadata": {
        "id": "XTQ1dGat-nzd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agent & Policy Transformer"
      ],
      "metadata": {
        "id": "r_c_vG3N2vWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(arr, tau=1.0):\n",
        "  arr = np.array(arr, dtype=np.float64)\n",
        "  arr /= tau\n",
        "  m = max(arr)\n",
        "  z = np.exp(arr - m)\n",
        "  return z / z.sum()"
      ],
      "metadata": {
        "id": "_ozPx22O2Z1R"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def agent(pi, epsilon=0):\n",
        "  # pi must return array of non-negative values\n",
        "  def c(game):\n",
        "    w, h = game.w, game.h\n",
        "    m, p = np.ones(game.w), np.array(pi(game))\n",
        "    for i in range(game.w):\n",
        "      if game.board[(i + 1) * h - 1] != 0: m[i], p[i] = 0, 0\n",
        "    s = p.sum()\n",
        "    if np.random.uniform() < epsilon or s == 0:\n",
        "      s = m.sum()\n",
        "      if s == 0: return None # Cannot do anything\n",
        "      else: return np.random.choice(w, p=(m / s))\n",
        "    else: return np.random.choice(w, p=(p / s))\n",
        "  name = str(pi)\n",
        "  if hasattr(pi, 'name'): name = pi.name\n",
        "  c.name = \"stochastic({})\".format(name)\n",
        "  return c"
      ],
      "metadata": {
        "id": "t-gSuOtD2b-k"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pt_softmax(policy, tau=1.0):\n",
        "  def p(game):\n",
        "    p = policy(game)\n",
        "    return softmax(p, tau=tau)\n",
        "  p_name = mock4.agent_name(policy)\n",
        "  p.name = 'pt_softmax({},tau={})'.format(p_name, tau)\n",
        "  return p"
      ],
      "metadata": {
        "id": "NbRRi5JI2gOA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "아래 TSwB는 이전 챕터의 함수와 거의 동일하지만,\n",
        "`max_depth`와 `max_breadth`를 받는 대신에\n",
        "탐색할 detph별로 최대 너비를 list로 입력받습니다.\n",
        "예를 들어서 처음에는 7가지 경우 모두, 그 다음에는 3가지,\n",
        "이후 1자기 경로로 깊이 5만큼 탐색하고 싶다면,\n",
        "`[7, 3, 1, 1, 1, 1, 1]`을 전달하면 됩니다."
      ],
      "metadata": {
        "id": "l52OXk93M_lc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tree Search with Bound\n",
        "def pt_tswb(policy, max_breadths):\n",
        "  def search(game, depth): # return (winner, action)\n",
        "    # If game is terminated, just return the winner\n",
        "    w = game.check_win()\n",
        "    if w is not None or depth >= len(max_breadths):\n",
        "      if depth == 0: return w, policy(game)\n",
        "      else: return w, None\n",
        "    # Search\n",
        "    # First, find policy\n",
        "    p = policy(game)\n",
        "    m_full = np.ones(len(p))\n",
        "    m_lose = np.ones(len(p))\n",
        "    m_win = np.zeros(len(p))\n",
        "    m_searched = np.zeros(len(p))\n",
        "    # Remove unavailable actions\n",
        "    for c in range(game.w):\n",
        "      if game.board[(c + 1) * game.h - 1] != 0:\n",
        "        m_full[c] = 0\n",
        "    # Sort policy\n",
        "    s = sorted([(-x, i) for i, x in enumerate(p * m_full)])\n",
        "    for mprob, i in s[: max_breadths[depth]]:\n",
        "      if -mprob <= 0: break\n",
        "      m_searched[i] = 1\n",
        "      game.place(i)\n",
        "      w, j = search(game, depth + 1)\n",
        "      game.undo()\n",
        "      if w == game.player: # If player win, we can win follow this.\n",
        "        m_win[i] = 1\n",
        "      elif w == 3 - game.player: # If player lose, we should not follow\n",
        "        m_lose[i] = 0\n",
        "    if (m_win * p).sum() > 0: # If there are winning way, use them\n",
        "      return game.player, m_win * p\n",
        "    if (m_lose * m_full * m_searched * p).sum() > 0: # If there are no losing way\n",
        "      return None, m_lose * m_full * m_searched * p\n",
        "    # Otherwise\n",
        "    return (3 - game.player), m_lose * m_full * p\n",
        "    \n",
        "  def p(game):\n",
        "    winner, p = search(game, 0)\n",
        "    if p is None: p = policy(game)\n",
        "    return p\n",
        "  p.name = 'pt_tswb({},b={})'.format(\n",
        "      mock4.agent_name(policy), max_breadths)\n",
        "  return p"
      ],
      "metadata": {
        "id": "5uyQFsLb2uX7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neural Network"
      ],
      "metadata": {
        "id": "Kb8XnOqB3ABr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Flatten(nn.Module):\n",
        "  def forward(self, x):\n",
        "    if len(x.shape) == 3: return x.view(-1)\n",
        "    else: return x.flatten(1, -1)"
      ],
      "metadata": {
        "id": "qpkBFYoM2-bz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural network는 이전 policy를 저장할 수 있도록\n",
        "아래와 같이 만듭니다."
      ],
      "metadata": {
        "id": "kdAjtcucNZL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## nn\n",
        "def new_nn():\n",
        "  W = 7\n",
        "  H = 6\n",
        "  net = nn.Sequential(\n",
        "      # 01\n",
        "      nn.Conv2d(3, 64, 3, padding='same'),\n",
        "      nn.ReLU(),\n",
        "      # 02\n",
        "      nn.Conv2d(64, 64, 3, padding='same'),\n",
        "      nn.MaxPool2d(2),\n",
        "      nn.ReLU(),\n",
        "      # 03\n",
        "      nn.Conv2d(64, 32, 3, padding='same'),\n",
        "      nn.ReLU(),\n",
        "      # 04\n",
        "      nn.Conv2d(32, 8, 3, padding='same'),\n",
        "      nn.ReLU(),\n",
        "      # Lin01\n",
        "      Flatten(),\n",
        "      nn.Linear(8 * (W // 2) * (H // 2), 20),\n",
        "      nn.ReLU(),\n",
        "      # Lin02\n",
        "      nn.Linear(20, W),\n",
        "      # Softmax\n",
        "      nn.Softmax(dim=-1)\n",
        "  ).to(device)\n",
        "  return net\n",
        "\n",
        "def init_nn():\n",
        "  global policy, policy_backs\n",
        "  policy = new_nn()\n",
        "  policy_backs = []\n",
        "  save_policy()\n",
        "\n",
        "def save_policy():\n",
        "  global policy_backs\n",
        "  n = new_nn()\n",
        "  n.load_state_dict(policy.state_dict())\n",
        "  policy_backs = (policy_backs + [n])[-10: ]"
      ],
      "metadata": {
        "id": "6WkjC3QJ3ENn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_model(net):\n",
        "  def c(game):\n",
        "    X = game.tensor().unsqueeze(dim=0).to(device)\n",
        "    with torch.no_grad():\n",
        "      p = net(X)\n",
        "    arr = p.squeeze().to('cpu').numpy()\n",
        "    return arr\n",
        "  c.name = 'model({:x})'.format(id(net))\n",
        "  return c"
      ],
      "metadata": {
        "id": "N8PqW_Rm3nyO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(policy, v):\n",
        "  # Original GD (minimization) : theta <- theta - alpha D J(theta)\n",
        "  # In policy gradient (maximization) : theta <- theta + alpha v D log pi\n",
        "  # => Negative log likelihood weighted by reward v\n",
        "  return - (v * torch.log(policy)).mean()"
      ],
      "metadata": {
        "id": "zSyOrvC93ak1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Stat:\n",
        "  def __init__(self, n=0, w=0, l=0, d=0): self.n, self.w, self.l, self.d = n, w, l, d\n",
        "  def dup(self):\n",
        "    return Stat(self.n, self.w, self.l, self.d)\n",
        "  def __sub__(self, other):\n",
        "    return Stat(self.n - other.n, self.w - other.w, self.l - other.l, self.d - other.d)"
      ],
      "metadata": {
        "id": "Blrb79vAI9Ar"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# REINFORCE\n",
        "def learn(\n",
        "    agent1_gen,\n",
        "    agent2_gen,\n",
        "    opt,\n",
        "    n_episode,\n",
        "    n_epoch,\n",
        "    gamma,\n",
        "    batch_size,\n",
        "    interval_stat\n",
        "):\n",
        "  epi = 0\n",
        "  stat = Stat()\n",
        "  last_stat = Stat()\n",
        "  last_stat_epi = 0\n",
        "  Xs, As, Vs = [], [], []\n",
        "  for epi in range(n_episode):\n",
        "    # Run Game\n",
        "    game = Mock4()\n",
        "    a1 = agent1_gen(epi)\n",
        "    a2 = agent2_gen(epi)\n",
        "    result = game.play(a1, a2, p_msg=False, p_res=False)\n",
        "    # Make reward\n",
        "    stat.n += 1\n",
        "    if result == 0: stat.d += 1\n",
        "    elif result == 1: stat.w += 1\n",
        "    else: stat.l += 1\n",
        "    w = 3 - game.player\n",
        "    reward = 1 if result != 0 else 0.3\n",
        "    # Append to Batch\n",
        "    while len(game.history) > 0:\n",
        "      a = int(game.undo() / game.h)\n",
        "      if game.player == w:\n",
        "        S0_p = game.tensor(player=w)\n",
        "        Xs.append(S0_p)\n",
        "        As.append(a)\n",
        "        Vs.append(reward)\n",
        "        S0_pf = S0_p.flip((1,))\n",
        "        Xs.append(S0_pf)\n",
        "        As.append(game.w - a - 1)\n",
        "        Vs.append(reward)\n",
        "      reward *= gamma\n",
        "    # If batch is full enough, perform gradient ascent\n",
        "    if len(Xs) >= batch_size:\n",
        "      # Tensor-fy\n",
        "      X = torch.stack(Xs).to(device)\n",
        "      A = torch.tensor(As).unsqueeze(dim=1).to(device)\n",
        "      V = torch.tensor(Vs, dtype=torch.float).to(device)\n",
        "      Xs, As, Vs = [], [], []\n",
        "      # Learn\n",
        "      loss_list = []\n",
        "      for e in range(n_epoch):\n",
        "        opt.zero_grad()\n",
        "        pi_s = policy(X)\n",
        "        pi_sa = pi_s.gather(1, A).squeeze(dim=1)\n",
        "        loss = loss_fn(pi_sa, V)\n",
        "        loss_list.append(loss.mean().item())\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "      # Print status\n",
        "      if epi - last_stat_epi >= interval_stat:\n",
        "        save_policy()\n",
        "        print(\"----------\")\n",
        "        print(\"Ep #{:<6d} Loss {:13.10f} -> {:13.10f}\".format(\n",
        "          epi, loss_list[0], loss_list[-1]))\n",
        "        print(\"  Win Rate {:8.4f}% ({}w + {}d + {}l = {})\".format(\n",
        "            100 * (stat.w + stat.d * 0.5) / stat.n,\n",
        "            stat.w, stat.d, stat.l, stat.n))\n",
        "        dstat = stat - last_stat\n",
        "        print(\"   WR Diff {:8.4f}% ({}w + {}d + {}l = {})\".format(\n",
        "            100 * (dstat.w + dstat.d * 0.5) / dstat.n,\n",
        "            dstat.w, dstat.d, dstat.l, dstat.n))\n",
        "        mock4.test_mock4(20, agent1_gen(epi), mock4.agent_greedy)\n",
        "        last_stat = stat.dup()\n",
        "        last_stat_epi = epi"
      ],
      "metadata": {
        "id": "-qsC3mbG3gML"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습 1 (v.s. Greedy)"
      ],
      "metadata": {
        "id": "z_eSHmzMRid9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " def run():\n",
        "  init_nn()\n",
        "\n",
        "  agent1 = lambda e: agent(policy_model(policy))\n",
        "  agent2 = lambda e: mock4.agent_greedy\n",
        "  opt = optim.Adam(policy.parameters(), lr=1e-3, weight_decay=1e-6)\n",
        "\n",
        "  learn(\n",
        "      agent1_gen = agent1,\n",
        "      agent2_gen = agent2,\n",
        "      opt = opt,\n",
        "      n_episode = 10000,\n",
        "      n_epoch = 2,\n",
        "      gamma = 0.99,\n",
        "      batch_size = 100,\n",
        "      interval_stat = 500)\n",
        "run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8fCssob4xfD",
        "outputId": "706263df-b988-498b-af52-af4ce5c95e5f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------\n",
            "Ep #506    Loss  1.2348499298 ->  1.2363787889\n",
            "  Win Rate   0.3945% (2w + 0d + 505l = 507)\n",
            "   WR Diff   0.3945% (2w + 0d + 505l = 507)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0e05a350))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #1012   Loss  1.1338516474 ->  1.1246111393\n",
            "  Win Rate   0.4936% (5w + 0d + 1008l = 1013)\n",
            "   WR Diff   0.5929% (3w + 0d + 503l = 506)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0e05a350))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #1512   Loss  0.7100552917 ->  0.6774426699\n",
            "  Win Rate   0.8592% (13w + 0d + 1500l = 1513)\n",
            "   WR Diff   1.6000% (8w + 0d + 492l = 500)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0e05a350))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #2015   Loss  0.3047559857 ->  0.2788357139\n",
            "  Win Rate   3.3482% (67w + 1d + 1948l = 2016)\n",
            "   WR Diff  10.8350% (54w + 1d + 448l = 503)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0e05a350))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 6 (0.300) / Dr 0 (0.000) / W2 14 (0.700)\n",
            "----------\n",
            "Ep #2516   Loss  0.4912139177 ->  0.4853442311\n",
            "  Win Rate   5.0060% (120w + 12d + 2385l = 2517)\n",
            "   WR Diff  11.6766% (53w + 11d + 437l = 501)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0e05a350))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 2 (0.100) / Dr 0 (0.000) / W2 18 (0.900)\n",
            "----------\n",
            "Ep #3017   Loss  0.3380557299 ->  0.3127323091\n",
            "  Win Rate   7.7535% (223w + 22d + 2773l = 3018)\n",
            "   WR Diff  21.5569% (103w + 10d + 388l = 501)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0e05a350))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 7 (0.350) / Dr 0 (0.000) / W2 13 (0.650)\n",
            "----------\n",
            "Ep #3517   Loss  0.1284803897 ->  0.1245952100\n",
            "  Win Rate  11.8960% (403w + 31d + 3084l = 3518)\n",
            "   WR Diff  36.9000% (180w + 9d + 311l = 500)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0e05a350))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 12 (0.600) / Dr 0 (0.000) / W2 8 (0.400)\n",
            "----------\n",
            "Ep #4020   Loss  0.1219745278 ->  0.1165569723\n",
            "  Win Rate  17.8811% (697w + 44d + 3280l = 4021)\n",
            "   WR Diff  59.7416% (294w + 13d + 196l = 503)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0e05a350))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 12 (0.600) / Dr 1 (0.050) / W2 7 (0.350)\n",
            "----------\n",
            "Ep #4523   Loss  0.3729707003 ->  0.3737123907\n",
            "  Win Rate  18.8660% (826w + 55d + 3643l = 4524)\n",
            "   WR Diff  26.7396% (129w + 11d + 363l = 503)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0e05a350))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 9 (0.450) / Dr 1 (0.050) / W2 10 (0.500)\n",
            "----------\n",
            "Ep #5026   Loss  0.0473011583 ->  0.0473515838\n",
            "  Win Rate  22.8665% (1116w + 67d + 3844l = 5027)\n",
            "   WR Diff  58.8469% (290w + 12d + 201l = 503)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0e05a350))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 13 (0.650) / Dr 0 (0.000) / W2 7 (0.350)\n",
            "----------\n",
            "Ep #5527   Loss  0.0424621180 ->  0.0395390131\n",
            "  Win Rate  27.8853% (1506w + 71d + 3951l = 5528)\n",
            "   WR Diff  78.2435% (390w + 4d + 107l = 501)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0e05a350))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 17 (0.850) / Dr 0 (0.000) / W2 3 (0.150)\n",
            "----------\n",
            "Ep #6030   Loss  0.0723767802 ->  0.0705923066\n",
            "  Win Rate  30.9153% (1824w + 81d + 4126l = 6031)\n",
            "   WR Diff  64.2147% (318w + 10d + 175l = 503)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0e05a350))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 15 (0.750) / Dr 0 (0.000) / W2 5 (0.250)\n",
            "----------\n",
            "Ep #6531   Loss  0.2948754728 ->  0.2872169614\n",
            "  Win Rate  33.5426% (2149w + 84d + 4299l = 6532)\n",
            "   WR Diff  65.1697% (325w + 3d + 173l = 501)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0e05a350))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 15 (0.750) / Dr 0 (0.000) / W2 5 (0.250)\n",
            "----------\n",
            "Ep #7031   Loss  0.0689934567 ->  0.0671737790\n",
            "  Win Rate  36.2912% (2506w + 92d + 4434l = 7032)\n",
            "   WR Diff  72.2000% (357w + 8d + 135l = 500)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0e05a350))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 17 (0.850) / Dr 0 (0.000) / W2 3 (0.150)\n",
            "----------\n",
            "Ep #7531   Loss  0.0157869235 ->  0.0156892017\n",
            "  Win Rate  39.9097% (2955w + 102d + 4475l = 7532)\n",
            "   WR Diff  90.8000% (449w + 10d + 41l = 500)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0e05a350))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 19 (0.950) / Dr 1 (0.050) / W2 0 (0.000)\n",
            "----------\n",
            "Ep #8031   Loss  0.0032004758 ->  0.0031368891\n",
            "  Win Rate  43.3454% (3430w + 103d + 4499l = 8032)\n",
            "   WR Diff  95.1000% (475w + 1d + 24l = 500)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0e05a350))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 19 (0.950) / Dr 0 (0.000) / W2 1 (0.050)\n",
            "----------\n",
            "Ep #8531   Loss  0.1139687747 ->  0.1064627990\n",
            "  Win Rate  46.3197% (3900w + 104d + 4528l = 8532)\n",
            "   WR Diff  94.1000% (470w + 1d + 29l = 500)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0e05a350))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 16 (0.800) / Dr 0 (0.000) / W2 4 (0.200)\n",
            "----------\n",
            "Ep #9033   Loss  0.0185533240 ->  0.0184843075\n",
            "  Win Rate  48.8377% (4358w + 108d + 4568l = 9034)\n",
            "   WR Diff  91.6335% (458w + 4d + 40l = 502)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0e05a350))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 20 (1.000) / Dr 0 (0.000) / W2 0 (0.000)\n",
            "----------\n",
            "Ep #9533   Loss  0.0263604876 ->  0.0263519529\n",
            "  Win Rate  51.4579% (4852w + 108d + 4574l = 9534)\n",
            "   WR Diff  98.8000% (494w + 0d + 6l = 500)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0e05a350))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 20 (1.000) / Dr 0 (0.000) / W2 0 (0.000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10,000번의 게임을 `agent_greedy`와 진행하며,\n",
        "5,000번째가 넘어갈 때 쯤부터 승률 50%를 넘어가는 것을\n",
        "확인할 수 있습니다."
      ],
      "metadata": {
        "id": "nSUURZalRk2q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 평가"
      ],
      "metadata": {
        "id": "iRke9AjyR9b3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mock4.test_mock4(100, mock4.agent_random, agent(policy_model(policy)))\n",
        "mock4.test_mock4(100, mock4.agent_greedy, agent(policy_model(policy)))"
      ],
      "metadata": {
        "id": "YQD2vdeg41gZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dedee62-e0c7-45e9-c50f-066ae8f9b224"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** Test\n",
            "* A1 = random\n",
            "* A2 = stochastic(model(7f9d0e05a350))\n",
            "Total = 100 games\n",
            "W1 30 (0.300) / Dr 0 (0.000) / W2 70 (0.700)\n",
            "** Test\n",
            "* A1 = greedy\n",
            "* A2 = stochastic(model(7f9d0e05a350))\n",
            "Total = 100 games\n",
            "W1 12 (0.120) / Dr 0 (0.000) / W2 88 (0.880)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "랜덤을 상대로 낮지 않은 승률을 보여주며,\n",
        "`agent_greey` 상대로도 비교적 높은 승률을 보장합니다."
      ],
      "metadata": {
        "id": "kfvzwrvySJBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mock4.test_mock4(30, mock4.agent_random, agent(pt_softmax(pt_tswb(policy_model(policy), [3, 3, 2, 2]), tau=1e-3)))\n",
        "mock4.test_mock4(30, mock4.agent_greedy, agent(pt_softmax(pt_tswb(policy_model(policy), [3, 3, 2, 2]), tau=1e-3)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLuei7DzOR2J",
        "outputId": "6b89a012-6641-4d06-fff5-a250c49a3d36"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** Test\n",
            "* A1 = random\n",
            "* A2 = stochastic(pt_softmax(pt_tswb(model(7f9d0e05a350),b=[3, 3, 2, 2]),tau=0.001))\n",
            "Total = 30 games\n",
            "W1 3 (0.100) / Dr 0 (0.000) / W2 27 (0.900)\n",
            "** Test\n",
            "* A1 = greedy\n",
            "* A2 = stochastic(pt_softmax(pt_tswb(model(7f9d0e05a350),b=[3, 3, 2, 2]),tau=0.001))\n",
            "Total = 30 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 30 (1.000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습한 모델에 tree search를 추가하였을 때\n",
        "더욱 강하다는 것을 확인할 수 있습니다."
      ],
      "metadata": {
        "id": "4oKaCTLDSZpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mock4.test_mock4(30,\n",
        "  agent(pt_softmax(pt_tswb(mock4.policy_greedy_connect, [3]), tau=1e-3)),\n",
        "  agent(policy_model(policy)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8g-p331KSjus",
        "outputId": "d17e24e6-9cb4-4315-f9c9-db6c816795d7"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** Test\n",
            "* A1 = stochastic(pt_softmax(pt_tswb(<function policy_greedy_connect at 0x7f9dd063bcb0>,b=[3]),tau=0.001))\n",
            "* A2 = stochastic(model(7f9d0e05a350))\n",
            "Total = 30 games\n",
            "W1 30 (1.000) / Dr 0 (0.000) / W2 0 (0.000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "만약 greedy connect에 tree search를 섞게 되면\n",
        "제대로 대응하지 못하는 것을 확인할 수 있습니다."
      ],
      "metadata": {
        "id": "1Y35xvHQTAZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mock4.test_mock4(30,\n",
        "  agent(pt_softmax(pt_tswb(mock4.policy_greedy_connect, [3, 3]), tau=1e-3)),\n",
        "  agent(pt_softmax(pt_tswb(policy_model(policy), [4, 3, 2, 2]), tau=1e-3)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NW3bO8-TRNv",
        "outputId": "762d1679-f9c4-4e61-da74-7fd2cb895914"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** Test\n",
            "* A1 = stochastic(pt_softmax(pt_tswb(<function policy_greedy_connect at 0x7f9dd063bcb0>,b=[3, 3]),tau=0.001))\n",
            "* A2 = stochastic(pt_softmax(pt_tswb(model(7f9d0e05a350),b=[5, 3, 2, 2]),tau=0.001))\n",
            "Total = 30 games\n",
            "W1 25 (0.833) / Dr 1 (0.033) / W2 4 (0.133)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "마찬가지로 model쪽에 tree search를 섞어도 승률이\n",
        "높게 나오지 않음을 확인할 수 있습니다.\n",
        "\n",
        "이를 통해 model이 `agent_greedy`에 상당히 과적합이\n",
        "되었을 것이라고 추측해볼 수 있습니다."
      ],
      "metadata": {
        "id": "3F71dvC3Tg7x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습 2 (v.s. Tree Search with Greedy)"
      ],
      "metadata": {
        "id": "djW5NDsQUDlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " def run():\n",
        "  init_nn()\n",
        "\n",
        "  agent1 = lambda e: agent(policy_model(policy))\n",
        "  agent2 = lambda e: agent(pt_softmax(pt_tswb(mock4.policy_greedy_connect, [3, 3]), tau=1e-3))\n",
        "  opt = optim.Adam(policy.parameters(), lr=1e-3, weight_decay=1e-6)\n",
        "\n",
        "  learn(\n",
        "      agent1_gen = agent1,\n",
        "      agent2_gen = agent2,\n",
        "      opt = opt,\n",
        "      n_episode = 10000,\n",
        "      n_epoch = 2,\n",
        "      gamma = 0.99,\n",
        "      batch_size = 100,\n",
        "      interval_stat = 500)\n",
        "run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00LdiidSUFNj",
        "outputId": "9e27716b-5e84-4a03-9fd3-a968a3b30b5a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------\n",
            "Ep #507    Loss  1.7981696129 ->  1.7996817827\n",
            "  Win Rate   0.0000% (0w + 0d + 508l = 508)\n",
            "   WR Diff   0.0000% (0w + 0d + 508l = 508)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d077427d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #1013   Loss  1.6840283871 ->  1.6807372570\n",
            "  Win Rate   0.1972% (2w + 0d + 1012l = 1014)\n",
            "   WR Diff   0.3953% (2w + 0d + 504l = 506)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d077427d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #1517   Loss  1.7070105076 ->  1.7013781071\n",
            "  Win Rate   0.2635% (4w + 0d + 1514l = 1518)\n",
            "   WR Diff   0.3968% (2w + 0d + 502l = 504)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d077427d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 1 (0.050) / Dr 0 (0.000) / W2 19 (0.950)\n",
            "----------\n",
            "Ep #2023   Loss  1.5274075270 ->  1.5214676857\n",
            "  Win Rate   0.4447% (8w + 2d + 2014l = 2024)\n",
            "   WR Diff   0.9881% (4w + 2d + 500l = 506)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d077427d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #2524   Loss  1.3890624046 ->  1.3687078953\n",
            "  Win Rate   0.5545% (13w + 2d + 2510l = 2525)\n",
            "   WR Diff   0.9980% (5w + 0d + 496l = 501)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d077427d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #3027   Loss  1.5572012663 ->  1.5429897308\n",
            "  Win Rate   0.5945% (17w + 2d + 3009l = 3028)\n",
            "   WR Diff   0.7952% (4w + 0d + 499l = 503)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d077427d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #3534   Loss  1.4608361721 ->  1.4355281591\n",
            "  Win Rate   0.9901% (34w + 2d + 3499l = 3535)\n",
            "   WR Diff   3.3531% (17w + 0d + 490l = 507)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d077427d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 2 (0.100) / Dr 0 (0.000) / W2 18 (0.900)\n",
            "----------\n",
            "Ep #4036   Loss  1.2495700121 ->  1.2442612648\n",
            "  Win Rate   1.2262% (48w + 3d + 3986l = 4037)\n",
            "   WR Diff   2.8884% (14w + 1d + 487l = 502)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d077427d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 1 (0.050) / Dr 0 (0.000) / W2 19 (0.950)\n",
            "----------\n",
            "Ep #4538   Loss  1.2426742315 ->  1.2381668091\n",
            "  Win Rate   1.2778% (56w + 4d + 4479l = 4539)\n",
            "   WR Diff   1.6932% (8w + 1d + 493l = 502)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d077427d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #5044   Loss  1.2436156273 ->  1.2188872099\n",
            "  Win Rate   1.4272% (70w + 4d + 4971l = 5045)\n",
            "   WR Diff   2.7668% (14w + 0d + 492l = 506)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d077427d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #5545   Loss  1.2544635534 ->  1.2327860594\n",
            "  Win Rate   1.6138% (87w + 5d + 5454l = 5546)\n",
            "   WR Diff   3.4930% (17w + 1d + 483l = 501)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d077427d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #6046   Loss  1.1109148264 ->  1.0991970301\n",
            "  Win Rate   1.8604% (110w + 5d + 5932l = 6047)\n",
            "   WR Diff   4.5908% (23w + 0d + 478l = 501)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d077427d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 1 (0.050) / Dr 0 (0.000) / W2 19 (0.950)\n",
            "----------\n",
            "Ep #6551   Loss  1.2736296654 ->  1.2644561529\n",
            "  Win Rate   2.2360% (143w + 7d + 6402l = 6552)\n",
            "   WR Diff   6.7327% (33w + 2d + 470l = 505)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d077427d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 1 (0.050) / Dr 0 (0.000) / W2 19 (0.950)\n",
            "----------\n",
            "Ep #7055   Loss  1.2291443348 ->  1.2124916315\n",
            "  Win Rate   2.5227% (173w + 10d + 6873l = 7056)\n",
            "   WR Diff   6.2500% (30w + 3d + 471l = 504)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d077427d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 1 (0.050) / Dr 0 (0.000) / W2 19 (0.950)\n",
            "----------\n",
            "Ep #7559   Loss  1.0493837595 ->  1.0436446667\n",
            "  Win Rate   2.9233% (215w + 12d + 7333l = 7560)\n",
            "   WR Diff   8.5317% (42w + 2d + 460l = 504)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d077427d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 1 (0.050) / Dr 0 (0.000) / W2 19 (0.950)\n",
            "----------\n",
            "Ep #8063   Loss  1.1282567978 ->  1.1082437038\n",
            "  Win Rate   3.2614% (255w + 16d + 7793l = 8064)\n",
            "   WR Diff   8.3333% (40w + 4d + 460l = 504)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d077427d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 1 (0.050) / Dr 1 (0.050) / W2 18 (0.900)\n",
            "----------\n",
            "Ep #8565   Loss  0.9140093923 ->  0.9024701118\n",
            "  Win Rate   3.6423% (302w + 20d + 8244l = 8566)\n",
            "   WR Diff   9.7610% (47w + 4d + 451l = 502)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d077427d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 1 (0.050) / Dr 0 (0.000) / W2 19 (0.950)\n",
            "----------\n",
            "Ep #9065   Loss  1.0671082735 ->  1.0471493006\n",
            "  Win Rate   4.0702% (357w + 24d + 8685l = 9066)\n",
            "   WR Diff  11.4000% (55w + 4d + 441l = 500)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d077427d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 2 (0.100) / Dr 0 (0.000) / W2 18 (0.900)\n",
            "----------\n",
            "Ep #9566   Loss  0.7994845510 ->  0.7858387232\n",
            "  Win Rate   4.4789% (411w + 35d + 9121l = 9567)\n",
            "   WR Diff  11.8762% (54w + 11d + 436l = 501)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d077427d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 1 (0.050) / Dr 0 (0.000) / W2 19 (0.950)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 평가"
      ],
      "metadata": {
        "id": "fjQTvK8UYRpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mock4.test_mock4(100, mock4.agent_random, agent(policy_model(policy)))\n",
        "mock4.test_mock4(100, mock4.agent_greedy, agent(policy_model(policy)))\n",
        "mock4.test_mock4(30,\n",
        "  agent(pt_softmax(pt_tswb(mock4.policy_greedy_connect, [3, 3]), tau=1e-3)),\n",
        "  agent(policy_model(policy)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JEfd5wKVXxf",
        "outputId": "e65ab38f-4324-4dcb-f54d-9e471eda6930"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** Test\n",
            "* A1 = random\n",
            "* A2 = stochastic(model(7f9d077427d0))\n",
            "Total = 100 games\n",
            "W1 6 (0.060) / Dr 0 (0.000) / W2 94 (0.940)\n",
            "** Test\n",
            "* A1 = greedy\n",
            "* A2 = stochastic(model(7f9d077427d0))\n",
            "Total = 100 games\n",
            "W1 79 (0.790) / Dr 1 (0.010) / W2 20 (0.200)\n",
            "** Test\n",
            "* A1 = stochastic(pt_softmax(pt_tswb(<function policy_greedy_connect at 0x7f9dd063bcb0>,b=[3, 3]),tau=0.001))\n",
            "* A2 = stochastic(model(7f9d077427d0))\n",
            "Total = 30 games\n",
            "W1 25 (0.833) / Dr 0 (0.000) / W2 5 (0.167)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tree search를 곁들인 greedy의 경우에는 일반적인 greedy에\n",
        "비해 더 강력하다보니 10,000번의 게임으로는\n",
        "충분히 학습이 되지 않는다는 것을 볼 수 있습니다.\n",
        "다만, 그래도 어느 정도 local optimum에는 가까워지는 듯\n",
        "하다는 것을 볼 수 있습니다.\n",
        "(아마 시간을 더 들이거나 complexity를 올리는 방식으로\n",
        "충분히 학습시키는 것이 가능할 것입니다.)\n",
        "\n",
        "Tree search에 대해서는 아주 좋은 성능을 보여주지는 않지만\n",
        "몇몇 이기는 방법을 익힌 것을 볼 수 있으며,\n",
        "`agent_greedy`나 `agent_random`을 상대할 때에\n",
        "아주 나쁜 성능을 보이지는 않는다는 점에서, 과적합이 심하지는\n",
        "않음을 확인할 수 있습니다."
      ],
      "metadata": {
        "id": "W7Jxy3ZzVpQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mock4.test_mock4(30,\n",
        "  agent(pt_softmax(pt_tswb(mock4.policy_greedy_connect, [3, 3]), tau=1e-3)),\n",
        "  agent(pt_softmax(pt_tswb(policy_model(policy), [3, 3]), tau=1e-3)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yaia0pskW7S-",
        "outputId": "295a03de-1504-4909-a6a8-4519fb573d20"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** Test\n",
            "* A1 = stochastic(pt_softmax(pt_tswb(<function policy_greedy_connect at 0x7f9dd063bcb0>,b=[3, 3]),tau=0.001))\n",
            "* A2 = stochastic(pt_softmax(pt_tswb(model(7f9d077427d0),b=[3, 3]),tau=0.001))\n",
            "Total = 30 games\n",
            "W1 4 (0.133) / Dr 1 (0.033) / W2 25 (0.833)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "추가적으로, 학습한 model에 tree search를 곁들이게\n",
        "되면 같은 깊이의 tree search를 하는 greedy를\n",
        "압도하는 것을 확인할 수 있습니다."
      ],
      "metadata": {
        "id": "Bcn9N6JFXEmq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습 3 (v.s. Random)"
      ],
      "metadata": {
        "id": "77x8W5NBXz0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " def run():\n",
        "  init_nn()\n",
        "\n",
        "  agent1 = lambda e: agent(policy_model(policy))\n",
        "  agent2 = lambda e: mock4.agent_random\n",
        "  opt = optim.Adam(policy.parameters(), lr=1e-3, weight_decay=1e-6)\n",
        "\n",
        "  learn(\n",
        "      agent1_gen = agent1,\n",
        "      agent2_gen = agent2,\n",
        "      opt = opt,\n",
        "      n_episode = 10000,\n",
        "      n_epoch = 2,\n",
        "      gamma = 0.99,\n",
        "      batch_size = 100,\n",
        "      interval_stat = 500)\n",
        "run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdrXKsmXYC3Z",
        "outputId": "0e0d4853-24db-4c8a-be40-e96cdd22d6fe"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------\n",
            "Ep #500    Loss  1.7342803478 ->  1.7342375517\n",
            "  Win Rate  49.7006% (248w + 2d + 251l = 501)\n",
            "   WR Diff  49.7006% (248w + 2d + 251l = 501)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d075e14d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #1004   Loss  1.7154291868 ->  1.7152107954\n",
            "  Win Rate  50.0498% (500w + 6d + 499l = 1005)\n",
            "   WR Diff  50.3968% (252w + 4d + 248l = 504)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d075e14d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #1508   Loss  1.7534102201 ->  1.7531008720\n",
            "  Win Rate  49.8012% (748w + 7d + 754l = 1509)\n",
            "   WR Diff  49.3056% (248w + 1d + 255l = 504)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d075e14d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #2008   Loss  1.6588441133 ->  1.6575635672\n",
            "  Win Rate  51.2444% (1025w + 9d + 975l = 2009)\n",
            "   WR Diff  55.6000% (277w + 2d + 221l = 500)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d075e14d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #2512   Loss  1.6991565228 ->  1.6990588903\n",
            "  Win Rate  52.2483% (1307w + 12d + 1194l = 2513)\n",
            "   WR Diff  56.2500% (282w + 3d + 219l = 504)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d075e14d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #3015   Loss  1.7086098194 ->  1.7074223757\n",
            "  Win Rate  52.6525% (1582w + 12d + 1422l = 3016)\n",
            "   WR Diff  54.6720% (275w + 0d + 228l = 503)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d075e14d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #3517   Loss  1.7011828423 ->  1.7006996870\n",
            "  Win Rate  52.7288% (1849w + 12d + 1657l = 3518)\n",
            "   WR Diff  53.1873% (267w + 0d + 235l = 502)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d075e14d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #4017   Loss  1.7698364258 ->  1.7694576979\n",
            "  Win Rate  53.0737% (2125w + 15d + 1878l = 4018)\n",
            "   WR Diff  55.5000% (276w + 3d + 221l = 500)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d075e14d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #4521   Loss  1.6564781666 ->  1.6557502747\n",
            "  Win Rate  53.4387% (2409w + 15d + 2098l = 4522)\n",
            "   WR Diff  56.3492% (284w + 0d + 220l = 504)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d075e14d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #5023   Loss  1.7438881397 ->  1.7418707609\n",
            "  Win Rate  54.2994% (2720w + 16d + 2288l = 5024)\n",
            "   WR Diff  62.0518% (311w + 1d + 190l = 502)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d075e14d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #5526   Loss  1.6752465963 ->  1.6741791964\n",
            "  Win Rate  54.6770% (3013w + 18d + 2496l = 5527)\n",
            "   WR Diff  58.4493% (293w + 2d + 208l = 503)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d075e14d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #6029   Loss  1.8048367500 ->  1.8046703339\n",
            "  Win Rate  54.6269% (3284w + 20d + 2726l = 6030)\n",
            "   WR Diff  54.0755% (271w + 2d + 230l = 503)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d075e14d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #6532   Loss  1.7319929600 ->  1.7268490791\n",
            "  Win Rate  54.9671% (3581w + 20d + 2932l = 6533)\n",
            "   WR Diff  59.0457% (297w + 0d + 206l = 503)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d075e14d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #7038   Loss  1.7225213051 ->  1.7213222980\n",
            "  Win Rate  55.3346% (3885w + 20d + 3134l = 7039)\n",
            "   WR Diff  60.0791% (304w + 0d + 202l = 506)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d075e14d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #7542   Loss  1.7716548443 ->  1.7709960938\n",
            "  Win Rate  55.5349% (4179w + 20d + 3344l = 7543)\n",
            "   WR Diff  58.3333% (294w + 0d + 210l = 504)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d075e14d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #8042   Loss  1.5651824474 ->  1.5621656179\n",
            "  Win Rate  55.8249% (4480w + 20d + 3543l = 8043)\n",
            "   WR Diff  60.2000% (301w + 0d + 199l = 500)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d075e14d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #8544   Loss  1.7159975767 ->  1.7148808241\n",
            "  Win Rate  56.0562% (4779w + 22d + 3744l = 8545)\n",
            "   WR Diff  59.7610% (299w + 2d + 201l = 502)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d075e14d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #9047   Loss  1.6630673409 ->  1.6598464251\n",
            "  Win Rate  56.3384% (5086w + 23d + 3939l = 9048)\n",
            "   WR Diff  61.1332% (307w + 1d + 195l = 503)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d075e14d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #9548   Loss  1.7717990875 ->  1.7694458961\n",
            "  Win Rate  56.6499% (5398w + 23d + 4128l = 9549)\n",
            "   WR Diff  62.2754% (312w + 0d + 189l = 501)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d075e14d0))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 평가"
      ],
      "metadata": {
        "id": "IMttLjNQYK6F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습 자체는 진행이 됩니다만, 랜덤과의 승률도 유의미하게\n",
        "오르지는 않는 것을 확인할 수 있습니다."
      ],
      "metadata": {
        "id": "lKpGlcDRYxqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습 4 (v.s. Self)\n",
        "\n",
        "현재 자기 자신과 대결할 경우 학습의 방향이 틀려서\n",
        "잘 안 될 수도 있으므로, backup된 policy들과 대결합니다."
      ],
      "metadata": {
        "id": "iArnC578Y5J4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " def run():\n",
        "  init_nn()\n",
        "\n",
        "  agent1 = lambda e: agent(policy_model(policy))\n",
        "  agent2 = lambda e: agent(policy_model(policy_backs[np.random.randint(len(policy_backs))]))\n",
        "  opt = optim.Adam(policy.parameters(), lr=1e-3, weight_decay=1e-6)\n",
        "\n",
        "  learn(\n",
        "      agent1_gen = agent1,\n",
        "      agent2_gen = agent2,\n",
        "      opt = opt,\n",
        "      n_episode = 10000,\n",
        "      n_epoch = 2,\n",
        "      gamma = 0.99,\n",
        "      batch_size = 100,\n",
        "      interval_stat = 500)\n",
        "run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lz4GF3EYZHmF",
        "outputId": "727a5f4a-0fe3-4839-fb1f-4804501bfb24"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------\n",
            "Ep #503    Loss  1.7701172829 ->  1.7690320015\n",
            "  Win Rate  50.0992% (251w + 3d + 250l = 504)\n",
            "   WR Diff  50.0992% (251w + 3d + 250l = 504)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0762ae50))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #1003   Loss  1.6913933754 ->  1.6903479099\n",
            "  Win Rate  50.3984% (503w + 6d + 495l = 1004)\n",
            "   WR Diff  50.7000% (252w + 3d + 245l = 500)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0762ae50))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #1504   Loss  1.6926116943 ->  1.6898909807\n",
            "  Win Rate  51.7276% (775w + 7d + 723l = 1505)\n",
            "   WR Diff  54.3912% (272w + 1d + 228l = 501)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0762ae50))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #2007   Loss  1.6310442686 ->  1.6245292425\n",
            "  Win Rate  52.6643% (1053w + 9d + 946l = 2008)\n",
            "   WR Diff  55.4672% (278w + 2d + 223l = 503)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0762ae50))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 2 (0.100) / Dr 0 (0.000) / W2 18 (0.900)\n",
            "----------\n",
            "Ep #2508   Loss  1.6252554655 ->  1.6237876415\n",
            "  Win Rate  53.1088% (1327w + 11d + 1171l = 2509)\n",
            "   WR Diff  54.8902% (274w + 2d + 225l = 501)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0762ae50))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #3011   Loss  1.6668941975 ->  1.6650755405\n",
            "  Win Rate  53.1707% (1596w + 11d + 1405l = 3012)\n",
            "   WR Diff  53.4791% (269w + 0d + 234l = 503)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0762ae50))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #3514   Loss  1.7077012062 ->  1.7064472437\n",
            "  Win Rate  53.6415% (1879w + 13d + 1623l = 3515)\n",
            "   WR Diff  56.4612% (283w + 2d + 218l = 503)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0762ae50))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #4018   Loss  1.8404703140 ->  1.8375520706\n",
            "  Win Rate  53.5581% (2146w + 13d + 1860l = 4019)\n",
            "   WR Diff  52.9762% (267w + 0d + 237l = 504)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0762ae50))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #4522   Loss  1.7189016342 ->  1.7168979645\n",
            "  Win Rate  53.9354% (2431w + 17d + 2075l = 4523)\n",
            "   WR Diff  56.9444% (285w + 4d + 215l = 504)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0762ae50))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #5024   Loss  1.6477247477 ->  1.6454018354\n",
            "  Win Rate  54.4677% (2728w + 18d + 2279l = 5025)\n",
            "   WR Diff  59.2629% (297w + 1d + 204l = 502)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0762ae50))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #5525   Loss  1.5803399086 ->  1.5759239197\n",
            "  Win Rate  54.8227% (3020w + 19d + 2487l = 5526)\n",
            "   WR Diff  58.3832% (292w + 1d + 208l = 501)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0762ae50))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #6028   Loss  1.4811323881 ->  1.4778833389\n",
            "  Win Rate  55.3906% (3329w + 21d + 2679l = 6029)\n",
            "   WR Diff  61.6302% (309w + 2d + 192l = 503)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0762ae50))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #6528   Loss  1.5764296055 ->  1.5729475021\n",
            "  Win Rate  55.6440% (3622w + 22d + 2885l = 6529)\n",
            "   WR Diff  58.7000% (293w + 1d + 206l = 500)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0762ae50))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #7029   Loss  1.6903694868 ->  1.6871700287\n",
            "  Win Rate  56.0313% (3928w + 22d + 3080l = 7030)\n",
            "   WR Diff  61.0778% (306w + 0d + 195l = 501)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0762ae50))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #7534   Loss  1.5817565918 ->  1.5741133690\n",
            "  Win Rate  56.4831% (4245w + 22d + 3268l = 7535)\n",
            "   WR Diff  62.7723% (317w + 0d + 188l = 505)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0762ae50))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #8039   Loss  1.4245343208 ->  1.4218722582\n",
            "  Win Rate  56.6045% (4540w + 22d + 3478l = 8040)\n",
            "   WR Diff  58.4158% (295w + 0d + 210l = 505)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0762ae50))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #8540   Loss  1.4160786867 ->  1.4138923883\n",
            "  Win Rate  56.9605% (4854w + 22d + 3665l = 8541)\n",
            "   WR Diff  62.6747% (314w + 0d + 187l = 501)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0762ae50))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #9040   Loss  1.5462824106 ->  1.5368496180\n",
            "  Win Rate  57.6153% (5198w + 22d + 3821l = 9041)\n",
            "   WR Diff  68.8000% (344w + 0d + 156l = 500)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0762ae50))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #9543   Loss  1.3184739351 ->  1.3091574907\n",
            "  Win Rate  57.9160% (5516w + 23d + 4005l = 9544)\n",
            "   WR Diff  63.3201% (318w + 1d + 184l = 503)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9d0762ae50))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mock4.test_mock4(100, mock4.agent_random, agent(policy_model(policy)))\n",
        "mock4.test_mock4(100, mock4.agent_greedy, agent(policy_model(policy)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAwIqZiraNiI",
        "outputId": "1a1466c5-60d2-4851-c1d0-93f394dab237"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** Test\n",
            "* A1 = random\n",
            "* A2 = stochastic(model(7f9d0762ae50))\n",
            "Total = 100 games\n",
            "W1 15 (0.150) / Dr 0 (0.000) / W2 85 (0.850)\n",
            "** Test\n",
            "* A1 = greedy\n",
            "* A2 = stochastic(model(7f9d0762ae50))\n",
            "Total = 100 games\n",
            "W1 98 (0.980) / Dr 0 (0.000) / W2 2 (0.020)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습 과정을 살펴보면 model이 계속해서 기존의 model에\n",
        "비해서는 더 강해지는 것을 볼 수 있으며,\n",
        "`agent_random`에서 비교적 높은 승률을 거두는 것으로\n",
        "최소한 임의의 선택을 하는 것은 아니라고 볼 수 있습니다.\n",
        "\n",
        "다만, 자신을 조금씩 개선을 하지만, 어떤 수가 최선에 가까운지를\n",
        "빠르게 파악하지는 못하기 때문에 학습이 상당히 더딘 것을 볼\n",
        "수 있으며, 이 때문에 `agent_greedy`는 거의 이기지\n",
        "못하는 것을 확인할 수 있습니다."
      ],
      "metadata": {
        "id": "AfADIwWIaQ-b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습 5 (v.s. Greedy, then, Tree Search with Greedy)\n",
        "\n",
        "이 방식으로 실행하면 Gradient explode가 발생하는지,\n",
        "발산하게 됩니다."
      ],
      "metadata": {
        "id": "VQargPAAcNVw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습 6 (v.s. Mix Greedy and Tree Search with Greedy)"
      ],
      "metadata": {
        "id": "RzceunPiia0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " def run():\n",
        "  init_nn()\n",
        "\n",
        "  opt = optim.Adam(policy.parameters(), lr=2e-3, weight_decay=1e-6)\n",
        "\n",
        "  agent1 = lambda e: agent(policy_model(policy))\n",
        "\n",
        "  op = [\n",
        "        mock4.agent_greedy,\n",
        "        agent(pt_softmax(pt_tswb(mock4.policy_greedy_connect, [3, 3]), tau=1e-3))\n",
        "  ]\n",
        "  agent2 = lambda e: op[np.random.randint(2)]\n",
        "\n",
        "  learn(\n",
        "      agent1_gen = agent1,\n",
        "      agent2_gen = agent2,\n",
        "      opt = opt,\n",
        "      n_episode = 10000,\n",
        "      n_epoch = 2,\n",
        "      gamma = 0.99,\n",
        "      batch_size = 200,\n",
        "      interval_stat = 500)\n",
        "\n",
        "run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXrNNnREcXEg",
        "outputId": "41b62204-1071-43c5-beb9-73521d7d67cd"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------\n",
            "Ep #501    Loss  1.6494978666 ->  1.6428221464\n",
            "  Win Rate   0.3984% (2w + 0d + 500l = 502)\n",
            "   WR Diff   0.3984% (2w + 0d + 500l = 502)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9cc4758950))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #1014   Loss  1.6123181581 ->  1.6070866585\n",
            "  Win Rate   0.6897% (7w + 0d + 1008l = 1015)\n",
            "   WR Diff   0.9747% (5w + 0d + 508l = 513)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9cc4758950))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #1525   Loss  1.6123936176 ->  1.5972832441\n",
            "  Win Rate   0.5242% (8w + 0d + 1518l = 1526)\n",
            "   WR Diff   0.1957% (1w + 0d + 510l = 511)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9cc4758950))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #2036   Loss  1.3911645412 ->  1.3772492409\n",
            "  Win Rate   0.5400% (11w + 0d + 2026l = 2037)\n",
            "   WR Diff   0.5871% (3w + 0d + 508l = 511)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9cc4758950))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #2550   Loss  1.1339126825 ->  1.1095348597\n",
            "  Win Rate   0.5292% (13w + 1d + 2537l = 2551)\n",
            "   WR Diff   0.4864% (2w + 1d + 511l = 514)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9cc4758950))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #3057   Loss  1.1441169977 ->  1.1268805265\n",
            "  Win Rate   0.6704% (20w + 1d + 3037l = 3058)\n",
            "   WR Diff   1.3807% (7w + 0d + 500l = 507)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9cc4758950))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #3557   Loss  0.9975886345 ->  0.9681029320\n",
            "  Win Rate   1.1804% (41w + 2d + 3515l = 3558)\n",
            "   WR Diff   4.3000% (21w + 1d + 478l = 500)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9cc4758950))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #4064   Loss  1.1150770187 ->  1.1021504402\n",
            "  Win Rate   1.5867% (63w + 3d + 3999l = 4065)\n",
            "   WR Diff   4.4379% (22w + 1d + 484l = 507)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9cc4758950))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 20 (1.000)\n",
            "----------\n",
            "Ep #4567   Loss  1.0642772913 ->  1.0456339121\n",
            "  Win Rate   2.0468% (92w + 3d + 4473l = 4568)\n",
            "   WR Diff   5.7654% (29w + 0d + 474l = 503)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9cc4758950))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 4 (0.200) / Dr 0 (0.000) / W2 16 (0.800)\n",
            "----------\n",
            "Ep #5069   Loss  1.0543829203 ->  1.0247849226\n",
            "  Win Rate   2.5740% (128w + 5d + 4937l = 5070)\n",
            "   WR Diff   7.3705% (36w + 2d + 464l = 502)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9cc4758950))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 2 (0.100) / Dr 0 (0.000) / W2 18 (0.900)\n",
            "----------\n",
            "Ep #5578   Loss  0.9349400401 ->  0.9202258587\n",
            "  Win Rate   3.5401% (192w + 11d + 5376l = 5579)\n",
            "   WR Diff  13.1631% (64w + 6d + 439l = 509)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9cc4758950))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 5 (0.250) / Dr 0 (0.000) / W2 15 (0.750)\n",
            "----------\n",
            "Ep #6083   Loss  0.9492998719 ->  0.9298248887\n",
            "  Win Rate   4.6351% (273w + 18d + 5793l = 6084)\n",
            "   WR Diff  16.7327% (81w + 7d + 417l = 505)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9cc4758950))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 3 (0.150) / Dr 1 (0.050) / W2 16 (0.800)\n",
            "----------\n",
            "Ep #6587   Loss  0.5715779662 ->  0.5406763554\n",
            "  Win Rate   5.6315% (358w + 26d + 6204l = 6588)\n",
            "   WR Diff  17.6587% (85w + 8d + 411l = 504)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9cc4758950))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 4 (0.200) / Dr 0 (0.000) / W2 16 (0.800)\n",
            "----------\n",
            "Ep #7094   Loss  0.8929470778 ->  0.8706577420\n",
            "  Win Rate   6.5610% (446w + 39d + 6610l = 7095)\n",
            "   WR Diff  18.6391% (88w + 13d + 406l = 507)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9cc4758950))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 5 (0.250) / Dr 1 (0.050) / W2 14 (0.700)\n",
            "----------\n",
            "Ep #7596   Loss  0.8294228911 ->  0.8211796284\n",
            "  Win Rate   7.4898% (546w + 46d + 7005l = 7597)\n",
            "   WR Diff  20.6175% (100w + 7d + 395l = 502)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9cc4758950))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 2 (0.100) / Dr 0 (0.000) / W2 18 (0.900)\n",
            "----------\n",
            "Ep #8102   Loss  0.7411192656 ->  0.7261710167\n",
            "  Win Rate   8.3611% (653w + 49d + 7401l = 8103)\n",
            "   WR Diff  21.4427% (107w + 3d + 396l = 506)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9cc4758950))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 10 (0.500) / Dr 0 (0.000) / W2 10 (0.500)\n",
            "----------\n",
            "Ep #8611   Loss  0.8193914890 ->  0.7980240583\n",
            "  Win Rate   9.6087% (800w + 55d + 7757l = 8612)\n",
            "   WR Diff  29.4695% (147w + 6d + 356l = 509)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9cc4758950))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 8 (0.400) / Dr 1 (0.050) / W2 11 (0.550)\n",
            "----------\n",
            "Ep #9114   Loss  0.8220019937 ->  0.7996554971\n",
            "  Win Rate  10.8228% (957w + 59d + 8099l = 9115)\n",
            "   WR Diff  31.6103% (157w + 4d + 342l = 503)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9cc4758950))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 6 (0.300) / Dr 0 (0.000) / W2 14 (0.700)\n",
            "----------\n",
            "Ep #9614   Loss  0.8122404814 ->  0.7893052101\n",
            "  Win Rate  11.8305% (1106w + 63d + 8446l = 9615)\n",
            "   WR Diff  30.2000% (149w + 4d + 347l = 500)\n",
            "** Test\n",
            "* A1 = stochastic(model(7f9cc4758950))\n",
            "* A2 = greedy\n",
            "Total = 20 games\n",
            "W1 12 (0.600) / Dr 0 (0.000) / W2 8 (0.400)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mock4.test_mock4(100, mock4.agent_random, agent(policy_model(policy)))\n",
        "mock4.test_mock4(100, mock4.agent_greedy, agent(policy_model(policy)))\n",
        "mock4.test_mock4(30,\n",
        "  agent(pt_softmax(pt_tswb(mock4.policy_greedy_connect, [3, 3]), tau=1e-3)),\n",
        "  agent(policy_model(policy)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNfqLPqqeKjS",
        "outputId": "d9b02b3f-697d-420e-df8a-462706d85d74"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** Test\n",
            "* A1 = random\n",
            "* A2 = stochastic(model(7f9cc4758950))\n",
            "Total = 100 games\n",
            "W1 4 (0.040) / Dr 0 (0.000) / W2 96 (0.960)\n",
            "** Test\n",
            "* A1 = greedy\n",
            "* A2 = stochastic(model(7f9cc4758950))\n",
            "Total = 100 games\n",
            "W1 69 (0.690) / Dr 0 (0.000) / W2 31 (0.310)\n",
            "** Test\n",
            "* A1 = stochastic(pt_softmax(pt_tswb(<function policy_greedy_connect at 0x7f9dd063bcb0>,b=[3, 3]),tau=0.001))\n",
            "* A2 = stochastic(model(7f9cc4758950))\n",
            "Total = 30 games\n",
            "W1 26 (0.867) / Dr 2 (0.067) / W2 2 (0.067)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 경우에는 학습 1, 2의 절충안적인 결과가 나오는 것을\n",
        "확인할 수 있다.\n",
        "빠르게 greedy를 이기도록 수렴하지는 않지만,\n",
        "반대로 tree search가 있는 greedy에 대해서도 어느 정도의\n",
        "데이터를 쌓는 것을 볼 수 있다."
      ],
      "metadata": {
        "id": "g85EaVtXeUEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 결론\n",
        "\n",
        "- 두 agent가 대립하는 경우에 강화학습을 하면\n",
        "상대방이 어떤 agent냐에 따라 수렴하는 속도나 수렴하는\n",
        "점이 상당히 달라지는 것 같습니다.\n",
        "- 위 학습에서는 이기는 사람이 두는 방법대로 두도록\n",
        "학습을 시키기 때문에, 상대방이 강하면 강할수록 실력이 더욱\n",
        "강해지는 것을 볼 수 있습니다.\n",
        "이 때문에 만약에 environment나 게임에 대한 정보가\n",
        "있다면 이것을 적극적으로 활용해서 최대한 강한 상대와 맞붙여\n",
        "학습을 시켜야 빠르게 실력을 올릴 수 있다고 생각합니다.\n",
        "- 과적합과 편향 문제가 상당히 심한 것을 볼 수 있습니다.\n",
        "위의 학습 1과 6을 비교해보면 greedy에 대한 승률은\n",
        "학습 1이 압도적으로 높지만, 여러 모델과 대결했을 때는\n",
        "학습 6이 전반적으로 더 좋은 성능을 보여줍니다.\n",
        "때문에 agent가 어느 정도 제한되어 있지 않다면,\n",
        "가능한 다양한 agent와 맞붙이며 학습을 하는 것이 필요해보입니다.\n",
        "이 때문에 replay memory를 두거나\n",
        "여러 agent를 저장해두고 리그 식으로 대결시켜 학습하는 것이\n",
        "아닌가 싶습니다.\n",
        "- Policy network도 그렇지만,\n",
        "parameteric model을 학습할 때에는\n",
        "무작정 데이터를 많이 넣기보다는 정말로 필요한 데이터만\n",
        "넣어서 학습을 시키는 것이 중요한 것 같습니다.\n",
        "이전 챕터 12의 코드와 여기의 차이점은 이전에는\n",
        "보드판을 승자 입장에서 본 것 뿐만 아니라 패자 입장에서도\n",
        "전달하였고, 모든 수를 전부 입력으로 주었다는 것인데,\n",
        "여기서는 그런 경우를 모두 쳐내어서 과적합이 발생할 정도로\n",
        "학습하는 것이 가능해졌습니다.\n",
        "- REINFORCE 알고리즘에서 objective function\n",
        "$J$중, 임의의 $s$, $a$를 통해 return $G$를 얻었을 때\n",
        "이것이 $J$에서 차지하는 비중 $j$는\n",
        "$$j(s, a) = G \\pi(s, a) $$\n",
        "로 나타나고, 이것의 미분을 적분하여 기댓값으로 나타내기 위해서\n",
        "$$j(s, a) = G \\frac{\\pi(s, a)}{\\pi(s, a)} \\pi(s, a)$$\n",
        "$$ \\nabla j = (G \\nabla \\log \\pi(s, a)) \\pi(s, a) $$\n",
        "라고 정리합니다.\n",
        "이 때문에 만약 G가 음수인 경우에는\n",
        "$\\lim_{x \\to 0^+}\\log x$와\n",
        "같은 꼴로 음의 무한대로 급격하게 발산해버리는 문제가 생깁니다.\n",
        "따라서 보상은 음수가 되면 안 되는데,\n",
        "이런 경우에는 흔히들 말하는 페널티를 주는 것이 매우 힘듭니다.\n",
        "(예를 들어 기존에는 이기면 1, 지면 -1을 보상/페널티로\n",
        "지급했는데, 여기서는 $[0,1]$로 스케일 했습니다.)\n",
        "이를 다르게 말하자면 위 학습은 '올바른 행동에 대한 확률을\n",
        "증가'시키는 것은 하지만, '잘못된 행동에 대한 확률을 감소'\n",
        "시키는 것은 하지 않습니다.\n",
        "그런데 위는 NLL과 같은 형태이며, NLL에서 class가 2개인\n",
        "경우에 아래와 같은 형태의 BCE Loss를 사용합니다.\n",
        "$$j = -y_i\\log(p) - (1 - y_i)\\log(1 - p)$$\n",
        "위에 따르면 만약 label $y_i$가 1이면 앞의 $\\log(p)$의\n",
        "곡선을 따라 $p$가 극대화하는 방향으로 이동하고,\n",
        "label $y_i$가 0이면 뒤의 $\\log(1 - p)$의 곡선을 따라\n",
        "$p$가 극소화하는 방향으로 이동합니다.\n",
        "위 함수를 $G$, $\\pi$에 대해 스케일해보면\n",
        "$$j = -\\frac{G + 1}{2}\\log(\\pi) - \\frac{1 - G}{2}\\log(1 - \\pi)$$\n",
        "와 같이 만들어볼 수 있을 것입니다.\n",
        "다만, 이 식을 사용해서 원래 objective function\n",
        "$J$가 최대화가 될지는 잘 모르겠습니다.\n"
      ],
      "metadata": {
        "id": "6GkrG70sRvFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 다음에는 다시 원래 문제였던 오목으로 넘어가서\n",
        "agent를 학습해봅니다."
      ],
      "metadata": {
        "id": "2xDF9vhCsZEk"
      }
    }
  ]
}