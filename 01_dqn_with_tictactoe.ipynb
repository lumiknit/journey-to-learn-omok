{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_dqn_with_tictactoe.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 01. DQN with TicTacToe\n",
        "\n",
        "Based on\n",
        "https://github.com/lumiknit/TicTacToe-RL\n",
        "\n",
        "오목 RL을 구현하기 전 틱택토를 먼저 다뤄봅니다."
      ],
      "metadata": {
        "id": "v9vWa8WdaLs4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 틱택토 구현\n",
        "\n",
        "우선 아래와 같이 TicTacToe 게임을 구현합니다.\n",
        "\n",
        "(lumiknit/TicTacToe-RL/ttt.py)\n",
        "\n",
        "아래 `Game`에 대해서 반드시 알아야 하는 것은 다음과 같습니다:\n",
        "\n",
        "- 초기화 시 1차원 배열의 `board`를 생성.\n",
        "- 0부터 8까지의 정수가 좌표가 됨. 각각 1행 1열, 1행 2열, ..., 3행 3열 순으로 할당.\n",
        "- `.place(idx)`로 돌을 놓는 것이 가능.\n",
        "- `.check_win()`으로 승패를 판단. 만약 둘 중 이긴 사람이 있으면 `1` 또는 `2` 반환, 무승부일경우 `0` 반환, 아직 승부가 안 났으면 `None` 반환.\n",
        "- `.play(agent1, agent2)`를 통해 게임을 실행 가능. 이 `agent`는 `Game`의 instance를 인수로 받아서 다음 돌이 놓일 좌표를 반환하는 함수면 충분. (`user_input` 참고.)\n",
        "- `.to_numpy`를 통해 `numpy.array`로 변환 가능."
      ],
      "metadata": {
        "id": "TzGDIYa3bnyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Game:\n",
        "  def __init__(self):\n",
        "    self.board = [0] * 9\n",
        "    self.turn = 1\n",
        "    self.turns = 0\n",
        "    self.foul = False\n",
        "\n",
        "  def clone(self):\n",
        "    g = Game()\n",
        "    for i in range(9):\n",
        "      g.board[i] = self.board[i]\n",
        "    g.turn = self.turn\n",
        "    g.turns = self.turns\n",
        "    return g\n",
        "\n",
        "  def rotate(self):\n",
        "    # Clone and rotate clockwise\n",
        "    g = Game()\n",
        "    for r in range(3):\n",
        "      for c in range(3):\n",
        "        g.board[3 * r + c] = self.board[3 * (2 - c) + r]\n",
        "    g.turn = self.turn\n",
        "    g.turns = self.turns\n",
        "    return g\n",
        "\n",
        "  def place(self, idx):\n",
        "    # Place stone at board[idx]\n",
        "    # Iff the player cannot place a stone at idx, return False\n",
        "    if self.board[idx] != 0:\n",
        "      self.foul = True\n",
        "      return False\n",
        "    self.board[idx] = self.turn\n",
        "    self.turn = 3 - self.turn\n",
        "    self.turns += 1\n",
        "    return True\n",
        "\n",
        "  def check(self, i0, i1, i2):\n",
        "    # Check whether 3 stones of a same color are placed in i0, i1, i2\n",
        "    v = self.board[i0]\n",
        "    if 0 != v and v == self.board[i1] and v == self.board[i2]:\n",
        "      return v\n",
        "    else: return None\n",
        "\n",
        "  def check_win(self):\n",
        "    # If some player 1 or 2 win, return 1 or 2\n",
        "    # If they draw, return 0\n",
        "    # Otherwise return None\n",
        "    if self.foul:\n",
        "      return 3 - self.turn\n",
        "    r = None\n",
        "    r = r or self.check(0, 1, 2) or self.check(3, 4, 5) or self.check(6, 7, 8)\n",
        "    r = r or self.check(0, 3, 6) or self.check(1, 4, 7) or self.check(2, 5, 8)\n",
        "    r = r or self.check(0, 4, 8) or self.check(2, 4, 6)\n",
        "    if self.turns >= 9: r = r or 0\n",
        "    return r\n",
        "  \n",
        "  def to_numpy(self, player):\n",
        "    # Create 3 * 3 * 3 np array\n",
        "    a = []\n",
        "    for i in range(3):\n",
        "      p = i\n",
        "      if player == 2: p = (3 - p) % 3\n",
        "      b = []\n",
        "      for r in range(3):\n",
        "        col = []\n",
        "        for c in range(3):\n",
        "          col.append(1. if self.board[r * 3 + c] == p else 0.)\n",
        "        b.append(col)\n",
        "      a.append(b)\n",
        "    return np.array(a)\n",
        "\n",
        "  def __str__(self):\n",
        "    ch_arr = ['.', 'O', 'X']\n",
        "    s = \"---\\nTurn: {} ({})\\n\".format(self.turn, ch_arr[self.turn])\n",
        "    s += \" | 0 1 2\\n-+------\"\n",
        "    for r in range(3):\n",
        "      s += \"\\n{}| \".format(r * 3)\n",
        "      for c in range(3):\n",
        "        s += \"{} \".format(ch_arr[self.board[r * 3 + c]])\n",
        "    return s\n",
        "\n",
        "  def play(self, f1, f2, no_print=False):\n",
        "    if no_print: p = lambda x: x\n",
        "    else: p = print\n",
        "    # Play game using two input functions\n",
        "    if f1 == None: f1 = user_input\n",
        "    if f2 == None: f2 = user_input\n",
        "    tfn = [None, f1, f2]\n",
        "    while self.turns < 9: # While empty cell exists\n",
        "      p(str(self))\n",
        "      # Take an index of cell to put a stone\n",
        "      x = tfn[self.turn](self)\n",
        "      p(\"{}P INPUT = {}\".format(self.turn, x))\n",
        "      # Check foul move.\n",
        "      if type(x) != int or x < 0 or x >= 9 or self.board[x] != 0:\n",
        "        p(str(self))\n",
        "        p(\"Player {} put a stone on a non-empty cell!, Player {} win!\" \\\n",
        "            .format(self.turn, 3 - self.turn))\n",
        "        return\n",
        "      # Place a stone\n",
        "      self.place(x)\n",
        "      # Check game is done\n",
        "      w = self.check_win()\n",
        "      if w == 0: # Draw\n",
        "        p(str(self))\n",
        "        p(\"Draw\")\n",
        "        return\n",
        "      elif w != None: # Player `w` win\n",
        "        p(str(self))\n",
        "        p(\"Player {} win!\".format(w))\n",
        "        return\n",
        "    p(str(self))\n",
        "    p(\"Draw\")\n",
        "\n",
        "def user_input(game):\n",
        "  i = None\n",
        "  while i == None:\n",
        "    try:\n",
        "      # Read a index of cell (0~8)\n",
        "      x = input(\"Idx(0~8) > \")\n",
        "      i = int(x)\n",
        "    except:\n",
        "      print(\"Wrong Input!\")\n",
        "  return i"
      ],
      "metadata": {
        "id": "VQniorx_ay5Z"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "만약 직접 플레이를 해보고 싶다면 아래를 실행해보세요!"
      ],
      "metadata": {
        "id": "8AsR9EDrcGfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Game().play() # Uncomment THIS!"
      ],
      "metadata": {
        "id": "oMG-nPMndpxq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "틱택토의 경우에는 사람들이 이미 잘 알고 있는 전략이 존재합니다!\n",
        "예를 들어서 아래와 같이 중앙/모서리를 공략하게 되면 비교적 손쉽게 무승부로 유도하는 것이 가능합니다.\n",
        "\n",
        "(lumiknit/TicTacToe-RL/ttt_alg.py)"
      ],
      "metadata": {
        "id": "F5S_DqHDdvM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def ttt_alg(game):\n",
        "  # Simple strategy to win TicTacToe\n",
        "  def c(i):\n",
        "    # Check there are 2 same-color stones in a row\n",
        "    v = max([game.board[x] for x in i])\n",
        "    if v == 0: return None\n",
        "    cnt = 0\n",
        "    e = None\n",
        "    for j in range(3):\n",
        "      if v == game.board[i[j]]: cnt += 1\n",
        "      else: e = i[j]\n",
        "    if cnt != 2 or game.board[e] != 0: return None\n",
        "    return e\n",
        "  # The 1st or 2nd stone must be at the center or a corner\n",
        "  if game.turns <= 1:\n",
        "    if game.board[4] == 0: return 4\n",
        "    else: return [0, 2, 6, 8][random.randrange(4)]\n",
        "  # Otherwise, find there are 2 stones in a row\n",
        "  r = c([0, 1, 2]) or c([3, 4, 5]) or c([6, 7, 8])\n",
        "  r = r or c([0, 3, 6]) or c([1, 4, 7]) or c([2, 5, 8])\n",
        "  r = r or c([0, 4, 8]) or c([2, 4, 6])\n",
        "  if r != None: return r\n",
        "  # Otherwise, just pick a random position\n",
        "  for x in [4, 0, 2, 8, 6, 1, 7, 3, 5]:\n",
        "    if game.board[x] == 0: return x"
      ],
      "metadata": {
        "id": "HMiVCXvteEQ0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 알고리즘을 이용하면 다음과 같은 플레이를 볼 수 있습니다.\n",
        "(보통 사람이 게임을 하는 것 같지 않나요?)"
      ],
      "metadata": {
        "id": "6kld0HpMeJNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Game().play(ttt_alg, ttt_alg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNtF-2h1eOuF",
        "outputId": "fa3356e3-9ad2-4ce7-85a6-794846865b31"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "Turn: 1 (O)\n",
            " | 0 1 2\n",
            "-+------\n",
            "0| . . . \n",
            "3| . . . \n",
            "6| . . . \n",
            "1P INPUT = 4\n",
            "---\n",
            "Turn: 2 (X)\n",
            " | 0 1 2\n",
            "-+------\n",
            "0| . . . \n",
            "3| . O . \n",
            "6| . . . \n",
            "2P INPUT = 2\n",
            "---\n",
            "Turn: 1 (O)\n",
            " | 0 1 2\n",
            "-+------\n",
            "0| . . X \n",
            "3| . O . \n",
            "6| . . . \n",
            "1P INPUT = 0\n",
            "---\n",
            "Turn: 2 (X)\n",
            " | 0 1 2\n",
            "-+------\n",
            "0| O . X \n",
            "3| . O . \n",
            "6| . . . \n",
            "2P INPUT = 8\n",
            "---\n",
            "Turn: 1 (O)\n",
            " | 0 1 2\n",
            "-+------\n",
            "0| O . X \n",
            "3| . O . \n",
            "6| . . X \n",
            "1P INPUT = 5\n",
            "---\n",
            "Turn: 2 (X)\n",
            " | 0 1 2\n",
            "-+------\n",
            "0| O . X \n",
            "3| . O O \n",
            "6| . . X \n",
            "2P INPUT = 3\n",
            "---\n",
            "Turn: 1 (O)\n",
            " | 0 1 2\n",
            "-+------\n",
            "0| O . X \n",
            "3| X O O \n",
            "6| . . X \n",
            "1P INPUT = 6\n",
            "---\n",
            "Turn: 2 (X)\n",
            " | 0 1 2\n",
            "-+------\n",
            "0| O . X \n",
            "3| X O O \n",
            "6| O . X \n",
            "2P INPUT = 1\n",
            "---\n",
            "Turn: 1 (O)\n",
            " | 0 1 2\n",
            "-+------\n",
            "0| O X X \n",
            "3| X O O \n",
            "6| O . X \n",
            "1P INPUT = 7\n",
            "---\n",
            "Turn: 2 (X)\n",
            " | 0 1 2\n",
            "-+------\n",
            "0| O X X \n",
            "3| X O O \n",
            "6| O O X \n",
            "Draw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "아래 코드를 실행해서 직접 알고리즘과 대결해보세요!"
      ],
      "metadata": {
        "id": "u71MD_vgeSmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Game().play(ttt_alg) # Uncomment THIS!"
      ],
      "metadata": {
        "id": "a4c1FvR5egyd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Design Q function for TicTacToe\n",
        "\n",
        "Q function은 현재 상태와 행동을 입력으로 받아서, 미래에 받을 잠재적인 보상을 출력하는 함수입니다.\n",
        "여기서 상태와 행동은 게임에서 일어날 수 있는 모든 조합은 수용할 수 있어야 하며,\n",
        "보상은 실제 보상이라기보다는,\n",
        "다른 선택과 비교할 수 있는 상대적인 지표라고 생각하는 편이\n",
        "낫습니다.\n",
        "\n",
        "예를 들어서 TicTacToe의 경우에는\n",
        "\n",
        "- 상태는 보드판이 됩니다. $3 \\times 3$ 크기의 보드판에, 각 칸이 빈칸이거나 검정색/흰색 돌이 놓일 수 있으므로, 최대 $3^{9} = 19683$개의 경우의 수가 있습니다.\n",
        "다만 실제로는 두 사람이 번갈아가면서 두기 때문에,\n",
        "게임 중 나오는 경우는 이보다 적습니다.\n",
        "- 행동은 돌을 각 보드판 어디에 두느냐가 됩니다.\n",
        "판의 크기가 $9$이므로\n",
        "행동의 경우의 수도 최대 $9$가 됩니다.\n",
        "- TicTacToe에서 명확하고 객관적으로 부여할 수 있는\n",
        "보상은 승패여부입니다. 예를 들어서 이기면 $r$,\n",
        "지면 $-r$만큼 보상을 부여하고, 이 보상을 최대화 시키면\n",
        "된다는 것입니다.\n",
        "\n",
        "따라서 여기서의 Q function은\n",
        "$$Q : \\textbf{3}^{9} \\times \\textbf{9} \\to \\mathbb{R}$$\n",
        "가 되겠네요.\n",
        "\n",
        "다만, 실제로 Q function을 구현할 때에\n",
        "행동을 입력으로 주기보다는,\n",
        "각 행동별 보상을 모두 출력에 포함시키는 것이 편해서,\n",
        "다음과 같이 만들게 됩니다.\n",
        "\n",
        "$$Q : \\textbf{3}^{9} \\to \\mathbb{R}^9$$\n",
        "\n",
        "만약 이 $Q$의 출력의 최대값이 되는 위치가,\n",
        "해당 상태에서 둘 수 있는 최선의 행동이라고 하면,\n",
        "실제 agent는 $Q$를 평가하여 어디가 최선인지 알아내면 충분합니다."
      ],
      "metadata": {
        "id": "lxZ3zKCjej1I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q-Table (Only checking feasibilty)\n",
        "\n",
        "만약 위 $Q$를 표로 구현한다고 해봅시다.\n",
        "그러면 `Q`에는 입력과 출력 개수의 곱인\n",
        "$3^{11} = 177147$개의 실수를 저장해야 합니다.\n",
        "Single precision float로 저장한다면\n",
        "692 kB 정도로 그다지 크지 않은 (사실 매우 작은)\n",
        "메모리가 필요합니다.\n",
        "\n",
        "이제 TicTacToe를 학습시킨다는 것이 `Q`라는 표에 값을 채워넣는 것으로 바뀌었습니다!\n",
        "\n",
        "그런데 TicTacToe는 2인 제로섬 유한 완전정보 게임이다보니, 한쪽에 필승법이 존재하며,\n",
        "이 때문에 저희는 `n`개 초과의 돌이 판에 놓인 경우\n",
        "승패를 알 수 있다면, 현재 `n`개의 돌이 놓은 상태에서\n",
        "아래와 같이 승부를 판단해볼 수 있습니다.\n",
        "\n",
        "- `Q` 자체는 해당 수를 두었을 때 최종 결과를 나타냅니다.\n",
        "- 만약 자신이 새로운 돌을 둔 상황을 `S'`이라고 할 때,\n",
        "`Q(S')`의 값 중 '승'이 있다면,\n",
        "상대방은 그 위치에 돌을 둠으로써 상대방이 이길 수 있습니다.\n",
        "- 반대로 `Q(S')`이 모두 '패'라면\n",
        "상대는 어디에 돌을 두든 상대방은 패할 수 밖에 없습니다.\n",
        "- 즉 현재 상태 `S`가 있을 때, `Q(S)`는\n",
        "`Q(S')` 중 '패'가 존재하면 '승', 그렇지 않다면 '패'라고 하면 문제가 없습니다.\n",
        "- 편의상 `Q`는 현재 흑 차례일 때, 승률이라고 합시다. 만약 백이 어떤 수를 두어야 할지 판단해야 한다면, 판 위의 모든 돌의 색을 뒤바꾸면 됩니다.\n",
        "\n",
        "위를 바탕으로 간단한 알고리즘을 작성해볼 수 있습니다.\n",
        "\n",
        "```\n",
        "let Q be table\n",
        "\n",
        "; Fill every finish state\n",
        "for each board S\n",
        "  if S has 3-in-row\n",
        "    Q[S][0..8] = 'WIN' if S has my 3-in-row else 'LOSE'\n",
        "  elseif there is no empty cell in S\n",
        "    Q[S][0..8] = 'WIN' ; Let's consider draw is better than lose...\n",
        "\n",
        "; Fill other states\n",
        "until Q is fully filed\n",
        "  for each board S\n",
        "    for each x in 0..8\n",
        "      if a stone was placed at x in S\n",
        "        Q[S][x] = 'LOSE' ; foul\n",
        "      let S' be a state after I place a stone at x on S\n",
        "      let S'' be S' flipped colors of every stone\n",
        "      if some of Q[S''] is unfilled\n",
        "        continue\n",
        "      if Q[S''] contains 'WIN'\n",
        "        Q[S][x] = 'LOSE'\n",
        "      else\n",
        "        Q[S][x] = 'WIN'\n",
        "```\n",
        "\n",
        "정말 다행인 점은 TicTacToe의 상태는 strictly increasing하다는 것입니다.\n",
        "즉, 돌의 개수가 항상 증가하므로,\n",
        "현재 상태는 다음 상태보다 돌의 개수가 적으며,\n",
        "다음 상태의 `Q`가 모두 결정되면 (채워져 있으면)\n",
        "현재 상태 역시 `Q`가 결정되므로,\n",
        "무조건 종료를 하게 됩니다.\n",
        "\n",
        "더 정확히는 맨 처음 반복문에서\n",
        "돌이 9개 놓인 판의 `Q`가 모두 결정이 되며,\n",
        "아래의 `until` 내부의 반복문은\n",
        "현재까지의 `Q`에 대해 추가로 결정할 수 있는 판을 모두 찾아\n",
        "`Q`를 갱신하므로,\n",
        "적어도 처음에는 8개 돌이 놓인 판의 값을 모두 결정하고,\n",
        "그 다음에는 7개 돌이 놓인 판의 값을 모두 결정하고, ...를\n",
        "반복하게 됩니다.\n",
        "때문에 모든 판의 경우의 수를 많아봐야 10번 훑으면\n",
        "위 프로그램은 종료되며,\n",
        "이 횟수는 1771470번으로, 길어봐야 10초면 해결이 됩니다."
      ],
      "metadata": {
        "id": "ZyCARUHZhYIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-Learning (Only checking feasibility)\n",
        "\n",
        "위 방법은 TicTacToe에 필승법이 존재하기 때문에\n",
        "가능합니다.\n",
        "만약에 상태가 strictly increasing이 아니어서 몇 번의 행동으로 원래 상태로 돌아온다면 적용할 수 없습니다.\n",
        "추가적으로 단순히 승/패가 아닌 복잡한 보상이 주어진다면\n",
        "테이블을 채울 다른 방법이 필요하게 됩니다.\n",
        "\n",
        "실제로는 정책 $\\pi$에 대해\n",
        "벨만 방정식과 Q-function은\n",
        "$$\n",
        "v_\\pi(s) = \\mathbb{E}_\\pi(R_{t+1} + \\gamma v_\\pi(S_{t+1}) \\mid S_t = s)\n",
        "$$\n",
        "$$\n",
        "q_\\pi(s, a) = \\mathbb{E}_\\pi(R_{t+1} + \\gamma q_\\pi(S_{t+1}, A_{t+1}) \\mid S_t = s, A_t = a)\n",
        "$$\n",
        "와 같이 나타나며, 이 중 optimial Q-function은\n",
        "$$\n",
        "q_*(s, a) = \\mathbb{E}(R_{t+1} + \\gamma \\max_{a' \\in \\mathcal{A}} q_*(S_{t+1}, a') \\mid S_t=s, A_t = a)\n",
        "$$\n",
        "\n",
        "Q-Learning을 하게 되면\n",
        "다음과 같은 식을 통해 수렴을 시키게 됩니다:\n",
        "\n",
        "$$\n",
        "Q(s, a) \\leftarrow (1 - \\alpha) Q(s, a) + \\alpha( R(s, a) + \\gamma \\max_{a' \\in A} Q(s', a'))\n",
        "$$\n",
        "\n",
        "위의 테이블의 값을 갱신하는 부분은\n",
        "여기서 보상인 $R$을 승/패에 따라 적당한 두 실수로 둔 것이고,\n",
        "discount factor $\\gamma$를 1로 두었다고 생각하면 됩니다."
      ],
      "metadata": {
        "id": "PK_pP3z4qf2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Q Network\n",
        "\n",
        "Q-function을 일일이 학습을 시키려면\n",
        "저 표를 모두 관리하면서 조금씩 수렴을 시켜야 하는데,\n",
        "단순하게 봐도 심각한 문제를 찾을 수 있습니다.\n",
        "\n",
        "- 표가 너무 큽니다. 예를 들어서 오목은 상태만 $3^{15\\times15} \\simeq 2^{356}$개가 되는데, 이 상태 개수만큼 실수를 저장하는 것도 불가능합니다.\n",
        "- 한번에 상태 및 행동 조합 중 한가지에 대해서만 갱신을 합니다. 위의 경우의 수와 맞물려서 학습이 언제 끝날지 감도 안 오게 만드는 주 원인입니다.\n",
        "\n",
        "그래서 표 대신에 샘플을 바탕으로 함수를 근사하는\n",
        "알고리즘을 생각하는 것이고, 여기에 deep learning을\n",
        "적용해보자는 것이 DQN의 기본적인 아이디어입니다.\n",
        "\n",
        "실제로 DQN을 학습시키기 위해서는 세세한 조정이 필요하지만,\n",
        "여기서는 간단하게 구현해보도록 합니다.\n",
        "\n",
        "(lumiknit/TicTacToe-RL/agent.py)"
      ],
      "metadata": {
        "id": "xWOuHh7Zj3UC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "우선 Torch를 불러옵니다."
      ],
      "metadata": {
        "id": "yr4IKf2H0P3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "5fThAae70Br4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습 전, `Game`을 `tensor`로 바꾸는 부분과\n",
        "랜덤 행동을 발생시키는 함수,\n",
        "돌을 둘 수 없는 곳의 칸을 $-\\infty$ 로 설정하는 함수를 만듭니다."
      ],
      "metadata": {
        "id": "_Co3Fas-0TVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def game_to_input(game):\n",
        "  n = game.to_numpy(game.turn)\n",
        "  r = np.random.rand(3, 3, 3) / 10.0\n",
        "  return torch.tensor((n + r), dtype=torch.double) \\\n",
        "      .reshape(3 * 3 * 3)\n",
        "\n",
        "def pick_random_move(game):\n",
        "  t = None\n",
        "  v = np.random.rand()\n",
        "  idx = np.random.randint(9)\n",
        "  for off in range(9):\n",
        "    rdx = (idx + off) % 9\n",
        "    if game.board[rdx] == 0:\n",
        "      t = rdx\n",
        "      g = game.clone()\n",
        "      g.place(rdx)\n",
        "      if g.check_win() == None: break\n",
        "  return t\n",
        "\n",
        "def cut_foul(game, qvs):\n",
        "  q = qvs.clone().detach()\n",
        "  for i in range(9):\n",
        "    if game.board[i] != 0:\n",
        "      q[i] = -100.0\n",
        "  return q"
      ],
      "metadata": {
        "id": "DrTzTVAB0ptn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 모델을 만듭니다.\n",
        "\n",
        "보드게임을 학습할 때는 CNN을 주로 사용한다는 것 같지만,\n",
        "TicTacToe는 크기가 매우 작으므로 그냥 dense network를 구성합니다."
      ],
      "metadata": {
        "id": "oNGJCFie07Z8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Linear(3 * 3 * 3, 243).double(),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(243, 36).double(),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(36, 36).double(),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(36, 3 * 3).double(),\n",
        ")\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "gamma = 0.95"
      ],
      "metadata": {
        "id": "C2_DQ3OU1FEL"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "다음에는 돌을 두었을 때 reward를 포함해서\n",
        "새롭게 갱신될 Q-function의 값을 계산하는 함수를 만듭니다."
      ],
      "metadata": {
        "id": "x0sWIwse1au0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_Q(game, idx):\n",
        "  # Calculate reward obtained when a stone is put on idx cell\n",
        "  if not game.place(idx): # Foul: Q = -1\n",
        "    return -1.0, game.turn\n",
        "  else:\n",
        "    w = game.check_win()\n",
        "    if w == None: # Not finished: Q = - max_a Q(s', a)\n",
        "      with torch.no_grad():\n",
        "        qvs2 = cut_foul(game, model(game_to_input(game)))\n",
        "      return (0 - gamma * torch.max(qvs2).item()), w\n",
        "    elif w == 0: # Draw: Q = 0\n",
        "      return 0.0, w\n",
        "    elif w == 3 - game.turn: # Win: Q = 1\n",
        "      return 1.0, w\n",
        "    else: # Lose: Q = -1\n",
        "      return -1.0, w"
      ],
      "metadata": {
        "id": "9P1MxhVa19g2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 본격적으로 학습을 시킵니다."
      ],
      "metadata": {
        "id": "kVkXIV4q2D7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def learn():\n",
        "  # Deep learning on Q-NN\n",
        "\n",
        "  learning_rate = 0.02\n",
        "\n",
        "  # I don't know why Adam optimizer does not work well\n",
        "  # optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "  # Use SGD+Momentum optimizer\n",
        "  optimizer = optim.SGD(\n",
        "      model.parameters(),\n",
        "      lr=learning_rate,\n",
        "      momentum=0.9,\n",
        "      weight_decay=1e-5)\n",
        "\n",
        "  # Exploration parameter\n",
        "  epsilon = 0.25\n",
        "\n",
        "  # # of epochs\n",
        "  n_epoch = 20000\n",
        "  # Epoch when epsilon start to decrease\n",
        "  dec_ep_epoch = 3000\n",
        "  # Epsilon decreasement factor\n",
        "  ep_dim = 0.9999\n",
        "  # Lower-bound of epsilon\n",
        "  ep_lb = 0.05\n",
        "  # Interval of printing learning state\n",
        "  print_interval = 200\n",
        "\n",
        "  # Loss statistics\n",
        "  n_loss = 0\n",
        "  acc_loss = 0\n",
        "\n",
        "  # Start running\n",
        "  for epoch in range(n_epoch):\n",
        "    # Make a new game board\n",
        "    game = Game()\n",
        "    while True:\n",
        "      # Calculate good answer by my strategy\n",
        "      alg_idx = ttt_alg(game)\n",
        "      # Calculate Q-val\n",
        "      qvs = model(game_to_input(game))\n",
        "      # Make an expected Q vector\n",
        "      Y = qvs.clone().detach()\n",
        "      # Explore all cells\n",
        "      for idx in range(9):\n",
        "        # Put a stone if we can\n",
        "        if game.board[idx] != 0: continue\n",
        "        g = game.clone()\n",
        "        # State transition & calculate reward\n",
        "        nq, w = calc_Q(g, idx)\n",
        "        # Put a reward into an expected Q vector\n",
        "        Y[idx] = nq\n",
        "      # Back propagation\n",
        "      optimizer.zero_grad()\n",
        "      loss = loss_fn(qvs, Y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      # Update loss\n",
        "      acc_loss += loss.item()\n",
        "      n_loss += 1\n",
        "      # Put a stone\n",
        "      # If rand() < epsilon, go to random\n",
        "      # if rand() < 2 epsilon, use my strategy\n",
        "      # otherwise, use Q-NN\n",
        "      idx = None\n",
        "      with torch.no_grad():\n",
        "        qvs = cut_foul(game, model(game_to_input(game)))\n",
        "      rnd = np.random.rand()\n",
        "      if rnd < epsilon:\n",
        "        idx = pick_random_move(game)\n",
        "      elif rnd < 2 * epsilon:\n",
        "        idx = ttt_alg(game)\n",
        "      else:\n",
        "        idx = torch.argmax(qvs).item()\n",
        "      # Put a stone and check the game is done\n",
        "      nq, w = calc_Q(game, idx)\n",
        "      if w != None:\n",
        "        break\n",
        "    # Print loss statistics\n",
        "    if epoch % print_interval == print_interval - 1:\n",
        "      print(\"{:6d}: e={:.4f}; L = {:8.4f}\" \\\n",
        "          .format(epoch + 1, epsilon, acc_loss / n_loss))\n",
        "      acc_loss = 0\n",
        "      n_loss = 0\n",
        "    # Decrease epsilon\n",
        "    if epoch > dec_ep_epoch:\n",
        "      epsilon *= ep_dim\n",
        "      if epsilon < ep_lb: epsilon = ep_lb\n"
      ],
      "metadata": {
        "id": "l3CshPNn2K_I"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RS1aYqTu2nmZ",
        "outputId": "d69824ea-0601-439e-a5cf-9df64ad5075f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 18000: e=0.0558; L =   0.0190\n",
            " 18200: e=0.0547; L =   0.0161\n",
            " 18400: e=0.0536; L =   0.0108\n",
            " 18600: e=0.0525; L =   0.0093\n",
            " 18800: e=0.0515; L =   0.0098\n",
            " 19000: e=0.0505; L =   0.0115\n",
            " 19200: e=0.0500; L =   0.0098\n",
            " 19400: e=0.0500; L =   0.0120\n",
            " 19600: e=0.0500; L =   0.0102\n",
            " 19800: e=0.0500; L =   0.0106\n",
            " 20000: e=0.0500; L =   0.0140\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 모델을 바탕으로 행동하는 agent는 다음과 같습니다."
      ],
      "metadata": {
        "id": "5j8KmZ3l2qIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_action(game):\n",
        "  global model\n",
        "  with torch.no_grad():\n",
        "    qv = cut_foul(game, model(game_to_input(game)))\n",
        "  return torch.argmax(qv).item()"
      ],
      "metadata": {
        "id": "1rMDjFdL30NS"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이를 바탕으로 실제 게임을 돌려보도록 하죠."
      ],
      "metadata": {
        "id": "4Cms-GzQ344r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Game().play(model_action, ttt_alg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6EourVq39av",
        "outputId": "5ccfb407-8425-4953-92ee-9bac8afcf6fd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "Turn: 1 (O)\n",
            " | 0 1 2\n",
            "-+------\n",
            "0| . . . \n",
            "3| . . . \n",
            "6| . . . \n",
            "1P INPUT = 8\n",
            "---\n",
            "Turn: 2 (X)\n",
            " | 0 1 2\n",
            "-+------\n",
            "0| . . . \n",
            "3| . . . \n",
            "6| . . O \n",
            "2P INPUT = 4\n",
            "---\n",
            "Turn: 1 (O)\n",
            " | 0 1 2\n",
            "-+------\n",
            "0| . . . \n",
            "3| . X . \n",
            "6| . . O \n",
            "1P INPUT = 1\n",
            "---\n",
            "Turn: 2 (X)\n",
            " | 0 1 2\n",
            "-+------\n",
            "0| . O . \n",
            "3| . X . \n",
            "6| . . O \n",
            "2P INPUT = 0\n",
            "---\n",
            "Turn: 1 (O)\n",
            " | 0 1 2\n",
            "-+------\n",
            "0| X O . \n",
            "3| . X . \n",
            "6| . . O \n",
            "1P INPUT = 2\n",
            "---\n",
            "Turn: 2 (X)\n",
            " | 0 1 2\n",
            "-+------\n",
            "0| X O O \n",
            "3| . X . \n",
            "6| . . O \n",
            "2P INPUT = 5\n",
            "---\n",
            "Turn: 1 (O)\n",
            " | 0 1 2\n",
            "-+------\n",
            "0| X O O \n",
            "3| . X X \n",
            "6| . . O \n",
            "1P INPUT = 3\n",
            "---\n",
            "Turn: 2 (X)\n",
            " | 0 1 2\n",
            "-+------\n",
            "0| X O O \n",
            "3| O X X \n",
            "6| . . O \n",
            "2P INPUT = 6\n",
            "---\n",
            "Turn: 1 (O)\n",
            " | 0 1 2\n",
            "-+------\n",
            "0| X O O \n",
            "3| O X X \n",
            "6| X . O \n",
            "1P INPUT = 7\n",
            "---\n",
            "Turn: 2 (X)\n",
            " | 0 1 2\n",
            "-+------\n",
            "0| X O O \n",
            "3| O X X \n",
            "6| X O O \n",
            "Draw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 이를 바탕으로 승률을 확인해봅시다.\n",
        "먼저 랜덤으로 돌을 두는 agent는 다음과 같습니다."
      ],
      "metadata": {
        "id": "d9dAJKKZ4AEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rand_action(game):\n",
        "  # Play TTT randomly\n",
        "  idx = np.random.randint(9)\n",
        "  for off in range(9):\n",
        "    rdx = (idx + off) % 9\n",
        "    if game.board[rdx] == 0: return rdx\n",
        "  return idx"
      ],
      "metadata": {
        "id": "Cc2ksFOH5Bp9"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "여기에 승률 측정용 함수,"
      ],
      "metadata": {
        "id": "LHb9PT945EOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_games(a, b, rounds):\n",
        "  a_win = 0\n",
        "  a_draw = 0\n",
        "  a_lose = 0\n",
        "  for i in range(rounds):\n",
        "    g = Game()\n",
        "    sw = False\n",
        "    if np.random.rand() < 0.5:\n",
        "      sw = False\n",
        "      g.play(a, b, True)\n",
        "    else:\n",
        "      sw = True\n",
        "      g.play(b, a, True)\n",
        "    w = g.check_win()\n",
        "    if w != None and w >= 1 and sw: w = 3 - w\n",
        "    if w == 1: a_win += 1\n",
        "    elif w == 2: a_lose += 1\n",
        "    else: a_draw += 1\n",
        "  print(\"RESULT: {} win / {} draw / {} lose // {} rounds\"\n",
        "    .format(a_win, a_draw, a_lose, rounds))"
      ],
      "metadata": {
        "id": "DdHZiwNL5HRd"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-- v.s. Random\")\n",
        "run_games(model_action, rand_action, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLPwqzmB5svM",
        "outputId": "d919579d-6877-40d7-815b-ef10eda9ebf9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- v.s. Random\n",
            "RESULT: 889 win / 82 draw / 29 lose // 1000 rounds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-- v.s. Algorithm\")\n",
        "run_games(model_action, ttt_alg, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvk1patx5u_Y",
        "outputId": "225c0eb4-b996-4c53-8ff2-b9d5349b5fea"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- v.s. Algorithm\n",
            "RESULT: 0 win / 1000 draw / 0 lose // 1000 rounds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nueral network에서 계산이 오래 걸리기는 하지만,\n",
        "약 20,000번의 게임을 진행하여 학습을 하였고,\n",
        "알고리즘의 경우 거의 모든 경우에 질 수 없는 방식이기 때문에\n",
        "알고리즘과는 호각으로 싸우며,\n",
        "랜덤에 대해서는 압도적으로 이기는 것을 볼 수 있습니다.\n",
        "다만, 모든 수에 대해 학습이 되지는 않기 때문에\n",
        "일부 패배하는 경우가 있음을 알 수 있습니다."
      ],
      "metadata": {
        "id": "wGpNOi0Q56IZ"
      }
    }
  ]
}