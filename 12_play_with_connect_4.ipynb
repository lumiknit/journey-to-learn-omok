{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "12_play_with_connect_4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 12. Play with Connect 4"
      ],
      "metadata": {
        "id": "wGl8Gxbhi2jU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect 4 (중력4목)\n",
        "\n",
        "이전까지 오목을 학습하려고 이런저런 시도를 했지만\n",
        "그리 좋은 결과를 얻지는 못했습니다.\n",
        "\n",
        "여기서는 상태와 입력을 더 적은 Connect 4에\n",
        "RL을 시도해봅니다."
      ],
      "metadata": {
        "id": "qotAfJwUi8rn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 고려할 점들\n",
        "\n",
        "- Value network 등 neural network에는\n",
        "직관 이상으로 많은 layer가 필요합니다.\n",
        "AlphaGo의 경우에도 Layer를 12개 가량 쌓아서 구성을\n",
        "하였고, Connect 4도 Layer 7개로 하루종일 학습시켜도\n",
        "value network만으로는 greedy algorithm을 넘기는\n",
        "쉽지 않다고 합니다.\n",
        "결국에는 complexity를 최대한 늘리고 resource를 부어서\n",
        "value나 policy를 학습해야되고, 그걸로 부족한 부분을\n",
        "최대한 가지를 쳐서 트리 탐색을 할 수 밖에 없는 듯 합니다.\n",
        "- 이전까지는 단순히 Monte Carlo 방식으로 value\n",
        "function만을 근사하려고 했습니다만,\n",
        "여기서는 더 정석적인 방법을 시도합니다."
      ],
      "metadata": {
        "id": "Dooi2RxTnQQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 환경설정"
      ],
      "metadata": {
        "id": "nPI9AnYTqNOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "여기서는 Mock4.py를 사용합니다.\n",
        "\n",
        "https://github.com/lumiknit/mock4.py"
      ],
      "metadata": {
        "id": "nmA-0WNAqUdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf mock4.py m4\n",
        "!git clone https://github.com/lumiknit/mock4.py.git\n",
        "!mv mock4.py m4\n",
        "!mv m4/mock4.py .\n",
        "from mock4 import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rIscheqqPD_",
        "outputId": "fa447aae-4eed-41f2-e60c-43b37354e25b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mock4.py'...\n",
            "remote: Enumerating objects: 10, done.\u001b[K\n",
            "remote: Counting objects:  10% (1/10)\u001b[K\rremote: Counting objects:  20% (2/10)\u001b[K\rremote: Counting objects:  30% (3/10)\u001b[K\rremote: Counting objects:  40% (4/10)\u001b[K\rremote: Counting objects:  50% (5/10)\u001b[K\rremote: Counting objects:  60% (6/10)\u001b[K\rremote: Counting objects:  70% (7/10)\u001b[K\rremote: Counting objects:  80% (8/10)\u001b[K\rremote: Counting objects:  90% (9/10)\u001b[K\rremote: Counting objects: 100% (10/10)\u001b[K\rremote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects:  12% (1/8)\u001b[K\rremote: Compressing objects:  25% (2/8)\u001b[K\rremote: Compressing objects:  37% (3/8)\u001b[K\rremote: Compressing objects:  50% (4/8)\u001b[K\rremote: Compressing objects:  62% (5/8)\u001b[K\rremote: Compressing objects:  75% (6/8)\u001b[K\rremote: Compressing objects:  87% (7/8)\u001b[K\rremote: Compressing objects: 100% (8/8)\u001b[K\rremote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 10 (delta 2), reused 6 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects:  10% (1/10)   \rUnpacking objects:  20% (2/10)   \rUnpacking objects:  30% (3/10)   \rUnpacking objects:  40% (4/10)   \rUnpacking objects:  50% (5/10)   \rUnpacking objects:  60% (6/10)   \rUnpacking objects:  70% (7/10)   \rUnpacking objects:  80% (8/10)   \rUnpacking objects:  90% (9/10)   \rUnpacking objects: 100% (10/10)   \rUnpacking objects: 100% (10/10), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mock5.py와 거의 비슷하게 사용하면 됩니다."
      ],
      "metadata": {
        "id": "3m3DrCZKqv-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Mock4().play(agent_greedy, agent_greedy, p_msg=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9W-6WU_qrGL",
        "outputId": "9186a5ee-8091-4524-ad12-4070785e96e8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------\n",
            "[ Turn  29 ; 2P ]\n",
            "| 0 1 2 3 4 5 6 |\n",
            "| O . O X X . . |\n",
            "| X O O X X . . |\n",
            "| O O O X X . . |\n",
            "| O X X O O . . |\n",
            "| X O O X X . . |\n",
            "| X O O O X . . |\n",
            "1P Win (<function agent_greedy at 0x7f5623d55710>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_mock4(100, agent_random, agent_greedy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtZWF-nPqsxA",
        "outputId": "d57dc13e-f5c3-4481-8b94-10398be970bb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** Test\n",
            "* A1 = <function agent_random at 0x7f5623d49290>\n",
            "* A2 = <function agent_greedy at 0x7f5623d55710>\n",
            "Total = 100 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 100 (1.000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "외에 pytorch, numpy를 불러옵니다."
      ],
      "metadata": {
        "id": "suCxkFJOqzXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: {}\".format(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAvWVH3mqeq-",
        "outputId": "6d58a64a-b2e2-4273-8983-ff6184e3655b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Flatten(nn.Module):\n",
        "  def forward(self, x):\n",
        "    if len(x.shape) == 3: return x.view(-1)\n",
        "    else: return x.flatten(1, -1)"
      ],
      "metadata": {
        "id": "FdmA1qwwr7Pz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Replay():\n",
        "  def __init__(self, size):\n",
        "    self.size = size\n",
        "    self.b = []\n",
        "\n",
        "  def remove_olds(self):\n",
        "    if len(self.b) > self.size:\n",
        "      self.b = self.b[-self.size :]\n",
        "  \n",
        "  def add(self, S0, A0, R0, S1):\n",
        "    self.b.append((S0, A0, R0, S1))\n",
        "    self.remove_olds()\n",
        "  \n",
        "  def sample(self, size):\n",
        "    Z = [None] * size\n",
        "    for i in range(size):\n",
        "      j = np.random.randint(len(self.b))\n",
        "      Z[i] = self.b[j]\n",
        "    S0 = [z[0] for z in Z]\n",
        "    A0s = [z[1] for z in Z]\n",
        "    R0 = [z[2] for z in Z]\n",
        "    S1 = [z[3] for z in Z]\n",
        "    return S0, A0s, R0, S1"
      ],
      "metadata": {
        "id": "Ae3rsizRt3o3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DQN\n",
        "\n",
        "Deep neural network를 이용해서\n",
        "Q-Learning을 합니다.\n",
        "\n",
        "- Action value function $q$를\n",
        "deep neural network로 구성합니다.\n",
        "- Policy $\\pi$는 $q$를 그대로 쓰되,\n",
        "action을 선택할 떄 argmax로 선택합니다.\n",
        "(Softmax로 확률처럼 바꿀 수는 있습니다만..)\n",
        "Policy improvement를 할 경우에\n",
        "$\\pi$를 $q$로 바꿉니다.\n",
        "- $\\alpha$, $\\epsilon$을 모두 decay합니다.\n",
        "- Batch normalization을 사용합니다.\n",
        "- Episode를 진행하며 replay memory를 누적시키고\n",
        "replay memory에서 샘플링한 batch로 학습시킵니다.\n"
      ],
      "metadata": {
        "id": "LWWO03jPrHsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## nn\n",
        "def new_nn():\n",
        "  W = 7\n",
        "  H = 6\n",
        "  net = nn.Sequential(\n",
        "      # 01\n",
        "      nn.Conv2d(3, 32, 3, padding='same'),\n",
        "      nn.BatchNorm2d(32),\n",
        "      nn.ReLU(),\n",
        "      # 02\n",
        "      nn.Conv2d(32, 64, 3, padding='same'),\n",
        "      nn.BatchNorm2d(64),\n",
        "      nn.MaxPool2d(2),\n",
        "      nn.ReLU(),\n",
        "      # 03\n",
        "      nn.Conv2d(64, 64, 3, padding='same'),\n",
        "      nn.BatchNorm2d(64),\n",
        "      nn.ReLU(),\n",
        "      # 04\n",
        "      nn.Conv2d(64, 64, 3, padding='same'),\n",
        "      nn.BatchNorm2d(64),\n",
        "      nn.ReLU(),\n",
        "      # 05\n",
        "      nn.Conv2d(64, 4, 3, padding='same'),\n",
        "      nn.BatchNorm2d(4),\n",
        "      nn.ReLU(),\n",
        "      # Lin01\n",
        "      Flatten(),\n",
        "      nn.Linear(4 * (W // 2) * (H // 2), 20),\n",
        "      nn.BatchNorm1d(20),\n",
        "      nn.ReLU(),\n",
        "      # Lin02\n",
        "      nn.Linear(20, W)\n",
        "  ).to(device)\n",
        "  return net\n",
        "\n",
        "def update_policy(policy, q_fn):\n",
        "  policy.load_state_dict(q_fn.state_dict())\n",
        "  q_fn.train()\n",
        "  policy.eval()\n",
        "\n",
        "def init_nn():\n",
        "  global policy, q_fn\n",
        "  policy = new_nn()\n",
        "  q_fn = new_nn()\n",
        "  update_policy(policy, q_fn)"
      ],
      "metadata": {
        "id": "Trw4_b6sqoP9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Policy = epsilon-greedy for q\n",
        "def agent_policy(epsilon):\n",
        "  def agent(game):\n",
        "    if np.random.uniform() < epsilon: return agent_random(game)\n",
        "    X = game.tensor().unsqueeze(dim=0).to(device)\n",
        "    M = game.tensor_full()\n",
        "    with torch.no_grad():\n",
        "      Q = policy(X)\n",
        "      Q = Q.squeeze(dim=0)\n",
        "    Q[M] = -float('inf')\n",
        "    A = torch.argmax(Q)\n",
        "    return A\n",
        "  return agent"
      ],
      "metadata": {
        "id": "TDs9bmrItrQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replay = Replay(65536)\n",
        "replay_terminal = Replay(2048)"
      ],
      "metadata": {
        "id": "IfPGpmsAt_eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-Learning\n",
        "def learn(\n",
        "    opt,\n",
        "    loss_fn,\n",
        "    n_episode,\n",
        "    n_epoch,\n",
        "    int_policy_update,\n",
        "    gamma,\n",
        "    alpha_fn,\n",
        "    epsilon_fn,\n",
        "    sz_sample,\n",
        "    sz_sample_terminal\n",
        "):\n",
        "  epi = 0\n",
        "  while epi < n_episode:\n",
        "    # -- Get parameters\n",
        "    alpha = alpha_fn(epi)\n",
        "    epsilon = epsilon_fn(epi)\n",
        "    # -- Run Game\n",
        "    game = Mock4()\n",
        "    result = game.play(agent_policy(epsilon), agent_policy(0), p_msg=False, p_res=False)\n",
        "    reward = 1\n",
        "    if result == 0: # Draw\n",
        "      reward = 0\n",
        "      result = 1\n",
        "    # -- Append to Replay\n",
        "    S1_p, S1_o = None, None\n",
        "    while len(game.history) > 0:\n",
        "      h = game.history[-1]\n",
        "      a = int(h / game.h)\n",
        "      del game.history[-1]\n",
        "      game.board[h] = 0\n",
        "      S0_p = game.tensor(player=result)\n",
        "      S0_o = game.tensor(player=(3 - result))\n",
        "      replay.add(S0_p, a, reward, S1_p)\n",
        "      replay.add(S0_o, a, -reward, S1_o)\n",
        "      if S1_p is None:\n",
        "        replay_terminal.add(S0_p, a, reward, S1_p)\n",
        "        replay_terminal.add(S0_o, a, -reward, S1_o)\n",
        "      S1_p, S1_o = S0_p, S0_o\n",
        "      reward = 0\n",
        "    # -- Sampling and learning\n",
        "    if len(replay.b) >= sz_sample:\n",
        "      S_0, As, Rs, S_1 = replay.sample(sz_sample)\n",
        "      # Append Terminal states\n",
        "      t_S_0, t_As, t_Rs, t_S_1 = replay_terminal.sample(sz_sample_terminal)\n",
        "      S_0 += t_S_0\n",
        "      As += t_As\n",
        "      Rs += t_Rs\n",
        "      S_1  += t_S_1\n",
        "      # Tensor-fy\n",
        "      X_0 = torch.stack(S_0).to(device)\n",
        "      R = torch.tensor(Rs, dtype=torch.float).to(device)\n",
        "      Sz_1 = [torch.zeros(3, game.w, game.h) if s is None else s for s in S_1]\n",
        "      Snone_1 = [s is None for s in S_1]\n",
        "      X_1 = torch.stack(Sz_1).to(device)\n",
        "      # Calc Curr Q\n",
        "      with torch.no_grad():\n",
        "        Q_0 = q_fn(X_0)\n",
        "        Q_1 = q_fn(X_1)\n",
        "      Qa_0 = Q_0[range(len(As)), As]\n",
        "      Qmax_1 = torch.max(Q_1, dim=1).values\n",
        "      # Q_0 <- Q_0 + alpha * (R + gamma * max Q_1 - Q_0) if not terminated\n",
        "      Qtgt_0 = Qa_0 + alpha * (R + gamma * Qmax_1 - Qa_0)\n",
        "      # Q_0 <- R otherwise\n",
        "      Qtgt_0[Snone_1] = R[Snone_1]\n",
        "      # Learn\n",
        "      loss_list = []\n",
        "      for e in range(n_epoch):\n",
        "        opt.zero_grad()\n",
        "        Q_0 = q_fn(X_0)\n",
        "        Qa_0 = Q_0[range(len(As)), As]\n",
        "        loss = loss_fn(Qa_0, Qtgt_0)\n",
        "        loss_list.append(loss.mean().item())\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "      epi += 1\n",
        "      print(\"Ep #{} (#Repl={}) Loss {:.8f}α -> {:.8f}α\".format(\n",
        "          epi, len(replay.b), loss_list[0] / alpha, loss_list[-1] / alpha))\n",
        "      # Update Policy\n",
        "      if (epi + 1) % int_policy_update == 0:\n",
        "        update_policy(policy, q_fn)\n",
        "    else: print(\"Accumulating Replay... (#={})\".format(len(replay.b)))"
      ],
      "metadata": {
        "id": "8toPGhWluLqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_nn()\n",
        "\n",
        "opt = optim.Adam(q_fn.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "loss_fn = nn.SmoothL1Loss()\n",
        "n_episode = 1000\n",
        "n_epoch = 50\n",
        "int_policy_update = 10\n",
        "alpha_fn = lambda n: 1 / (1 + n)\n",
        "gamma = 0.99\n",
        "epsilon_fn = lambda n: 0.3 * (0.99 ** n)\n",
        "sz_sample_terminal = 512\n",
        "sz_sample = 2048 - sz_sample_terminal\n",
        "\n",
        "learn(\n",
        "    opt=opt,\n",
        "    loss_fn=loss_fn,\n",
        "    n_episode=n_episode,\n",
        "    n_epoch=n_epoch,\n",
        "    int_policy_update=int_policy_update,\n",
        "    alpha_fn=alpha_fn,\n",
        "    gamma=gamma,\n",
        "    epsilon_fn=epsilon_fn,\n",
        "    sz_sample=sz_sample,\n",
        "    sz_sample_terminal=sz_sample_terminal\n",
        ")"
      ],
      "metadata": {
        "id": "ZTFnd-xWuNAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_mock4(100, agent_random, agent_policy(0))\n",
        "test_mock4(100, agent_greedy, agent_policy(0))"
      ],
      "metadata": {
        "id": "zcOmJ0Lyu2kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy Gradient"
      ],
      "metadata": {
        "id": "1STVPHqNu6an"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#WIP"
      ],
      "metadata": {
        "id": "YPKCQ4JmvdVx"
      }
    }
  ]
}