{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "12_play_with_connect_4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 12. Play with Connect 4"
      ],
      "metadata": {
        "id": "wGl8Gxbhi2jU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect 4 (중력4목)\n",
        "\n",
        "이전까지 오목을 학습하려고 이런저런 시도를 했지만\n",
        "그리 좋은 결과를 얻지는 못했습니다.\n",
        "\n",
        "여기서는 상태와 입력을 더 적은 Connect 4에\n",
        "RL을 시도해봅니다."
      ],
      "metadata": {
        "id": "qotAfJwUi8rn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 이전까지의 문제점\n",
        "\n",
        "강화학습을 할 때에는 최종적으로는 policy $\\pi$를\n",
        "탐색하는 것이 목표가 되며,\n",
        "이 $\\pi$를 직접 학습시키느냐 (policy-based)\n",
        "아니면 value function인 $v$나 $q$를 학습시켜서\n",
        "$\\pi$를 구하느냐 (value-based)로 나뉘게 됩니다.\n",
        "\n",
        "이전까지 시도했던 방법들은 $q$를 Monte Carlo\n",
        "policy evaluation으로 value function을\n",
        "근사를 하려고 했습니다.\n",
        "처음에는 $q$를 근사하려고 했지만, 오목판의\n",
        "상태와 행동이 너무 많아서 잘 안 되나 싶어서 $v$를\n",
        "근사해보고자 하였고, 실제로는 둘 다 잘 되지 않았습니다.\n",
        "\n",
        "다만, 이전 시도에는 몇가지\n",
        "문제가 있는데,\n",
        "\n",
        "- Policy를 $\\pi = \\text{argmax} \\circ q$라고\n",
        "두고 $q$를 갱신하다보니, policy가 unstable하게\n",
        "변하는 듯 합니다.\n",
        "- Value network의 complexity도,\n",
        "학습이 어느 정도 진행될 것이라고 생각한 time bound도\n",
        "너무 작았습니다.\n",
        "Connect 4는 오목보다는 상태나 행동이 적은 편인데,\n",
        "이 게임도 7개의 layer로 하루종일 학습시켜봐야\n",
        "value network만으로 사람을 이기기는 쉽지 않다고 합니다."
      ],
      "metadata": {
        "id": "SBW_PeS3u4HL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "그래서 여기서는 기존 오목 대신에 Connect4 (중력4목)에\n",
        "기존보다 complexity를 높인 neural network를 사용하여\n",
        "value-based learning과 policy-based learning을\n",
        "적용해보고 결과를 확인해봅니다."
      ],
      "metadata": {
        "id": "lYymk4Yjy7VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 환경설정"
      ],
      "metadata": {
        "id": "nPI9AnYTqNOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "여기서는 Mock4.py를 사용합니다.\n",
        "\n",
        "https://github.com/lumiknit/mock4.py"
      ],
      "metadata": {
        "id": "nmA-0WNAqUdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf mock4.py m4\n",
        "!git clone https://github.com/lumiknit/mock4.py.git\n",
        "!mv mock4.py m4\n",
        "!mv m4/mock4.py .\n",
        "from mock4 import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rIscheqqPD_",
        "outputId": "c9b85bad-6e7a-456b-881e-6c7dc58c03ef"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mock4.py'...\n",
            "remote: Enumerating objects: 13, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 13 (delta 3), reused 6 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (13/13), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mock5.py와 거의 비슷하게 사용하면 됩니다."
      ],
      "metadata": {
        "id": "3m3DrCZKqv-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Mock4().play(agent_greedy, agent_greedy, p_msg=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9W-6WU_qrGL",
        "outputId": "8fd111aa-c972-4710-c0c3-79d6d0a42def"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------\n",
            "[ Turn  29 ; 2P ]\n",
            "| 0 1 2 3 4 5 6 |\n",
            "| O . O X X . . |\n",
            "| X O O X X . . |\n",
            "| O O O X X . . |\n",
            "| O X X O O . . |\n",
            "| X O O X X . . |\n",
            "| X O O O X . . |\n",
            "1P Win (<function agent_greedy at 0x7f9ce4cfb710>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_mock4(100, agent_random, agent_greedy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtZWF-nPqsxA",
        "outputId": "2f7938dd-8271-4af5-8b46-69b5f9d05c0d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** Test\n",
            "* A1 = <function agent_random at 0x7f9ce4cfb0e0>\n",
            "* A2 = <function agent_greedy at 0x7f9ce4cfb710>\n",
            "Total = 100 games\n",
            "W1 0 (0.000) / Dr 0 (0.000) / W2 100 (1.000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "외에 pytorch, numpy를 불러옵니다."
      ],
      "metadata": {
        "id": "suCxkFJOqzXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: {}\".format(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAvWVH3mqeq-",
        "outputId": "30585c30-9a96-488d-95d1-8eca3755a8c5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Flatten(nn.Module):\n",
        "  def forward(self, x):\n",
        "    if len(x.shape) == 3: return x.view(-1)\n",
        "    else: return x.flatten(1, -1)"
      ],
      "metadata": {
        "id": "FdmA1qwwr7Pz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Replay():\n",
        "  def __init__(self, size):\n",
        "    self.size = size\n",
        "    self.b = []\n",
        "\n",
        "  def remove_olds(self):\n",
        "    if len(self.b) > self.size:\n",
        "      self.b = self.b[-self.size :]\n",
        "  \n",
        "  def add(self, S0, A0, R0, S1):\n",
        "    self.b.append((S0, A0, R0, S1))\n",
        "    self.remove_olds()\n",
        "  \n",
        "  def sample(self, size):\n",
        "    Z = [None] * size\n",
        "    for i in range(size):\n",
        "      j = np.random.randint(len(self.b))\n",
        "      Z[i] = self.b[j]\n",
        "    S0 = [z[0] for z in Z]\n",
        "    A0s = [z[1] for z in Z]\n",
        "    R0 = [z[2] for z in Z]\n",
        "    S1 = [z[3] for z in Z]\n",
        "    return S0, A0s, R0, S1"
      ],
      "metadata": {
        "id": "Ae3rsizRt3o3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scope, to separate namespace\n",
        "class Scope(): pass"
      ],
      "metadata": {
        "id": "e5hz5pslwtos"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DQN\n",
        "\n",
        "Deep neural network를 이용해서\n",
        "Q-Learning을 합니다.\n",
        "\n",
        "- Action value function $q$를\n",
        "deep neural network로 구성합니다.\n",
        "- Policy $\\pi$는 $q$를 그대로 쓰되,\n",
        "action을 선택할 떄 argmax로 선택합니다.\n",
        "(Softmax로 확률처럼 바꿀 수는 있습니다만..)\n",
        "Policy improvement는 특정 개수의 episode가 \n",
        "진행되면\n",
        "$\\pi$를 $q$로 바꾸는 식으로 이루어집니다.\n",
        "- $\\alpha$, $\\epsilon$을 모두 decay합니다.\n",
        "- Batch normalization을 사용합니다.\n",
        "- Episode를 진행하며 replay memory를 누적시키고\n",
        "replay memory에서 샘플링한 batch로 학습시킵니다."
      ],
      "metadata": {
        "id": "LWWO03jPrHsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ql = Scope()"
      ],
      "metadata": {
        "id": "q6ZBmFdaxhGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## nn\n",
        "def _new_nn():\n",
        "  W = 7\n",
        "  H = 6\n",
        "  net = nn.Sequential(\n",
        "      # 01\n",
        "      nn.Conv2d(3, 64, 3, padding='same'),\n",
        "      nn.BatchNorm2d(64),\n",
        "      nn.ReLU(),\n",
        "      # 02\n",
        "      nn.Conv2d(64, 64, 3, padding='same'),\n",
        "      nn.BatchNorm2d(64),\n",
        "      nn.MaxPool2d(2),\n",
        "      nn.ReLU(),\n",
        "      # 03\n",
        "      nn.Conv2d(64, 32, 3, padding='same'),\n",
        "      nn.BatchNorm2d(32),\n",
        "      nn.ReLU(),\n",
        "      # 04\n",
        "      nn.Conv2d(32, 32, 3, padding='same'),\n",
        "      nn.BatchNorm2d(32),\n",
        "      nn.ReLU(),\n",
        "      # 05\n",
        "      nn.Conv2d(32, 8, 3, padding='same'),\n",
        "      nn.BatchNorm2d(8),\n",
        "      nn.ReLU(),\n",
        "      # Lin01\n",
        "      Flatten(),\n",
        "      nn.Linear(8 * (W // 2) * (H // 2), 20),\n",
        "      nn.BatchNorm1d(20),\n",
        "      nn.ReLU(),\n",
        "      # Lin02\n",
        "      nn.Linear(20, W)\n",
        "  ).to(device)\n",
        "  return net\n",
        "ql.new_nn = _new_nn\n",
        "\n",
        "def _update_policy(policy, q):\n",
        "  policy.load_state_dict(q.state_dict())\n",
        "  q.train()\n",
        "  policy.eval()\n",
        "ql.update_policy = _update_policy\n",
        "\n",
        "def _init_nn():\n",
        "  ql.policy = ql.new_nn()\n",
        "  ql.q = ql.new_nn()\n",
        "  ql.update_policy(ql.policy, ql.q)\n",
        "ql.init_nn = _init_nn"
      ],
      "metadata": {
        "id": "Trw4_b6sqoP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Policy = epsilon-greedy for q\n",
        "def _agent_policy(epsilon):\n",
        "  def agent(game):\n",
        "    if np.random.uniform() < epsilon: return agent_random(game)\n",
        "    X = game.tensor().unsqueeze(dim=0).to(device)\n",
        "    M = game.tensor_full()\n",
        "    with torch.no_grad():\n",
        "      Q = ql.policy(X)\n",
        "      Q = Q.squeeze(dim=0)\n",
        "    Q[M] = -float('inf')\n",
        "    A = torch.argmax(Q)\n",
        "    return A\n",
        "  return agent\n",
        "ql.agent_policy = _agent_policy"
      ],
      "metadata": {
        "id": "TDs9bmrItrQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ql.replay = Replay(65536)\n",
        "ql.replay_terminal = Replay(2048)"
      ],
      "metadata": {
        "id": "IfPGpmsAt_eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-Learning\n",
        "def _learn(\n",
        "    opt,\n",
        "    loss_fn,\n",
        "    n_episode,\n",
        "    n_epoch,\n",
        "    int_policy_update,\n",
        "    gamma,\n",
        "    alpha_fn,\n",
        "    epsilon_fn,\n",
        "    sz_sample,\n",
        "    sz_sample_terminal\n",
        "):\n",
        "  init_left = 20\n",
        "  epi = 0\n",
        "  while epi < n_episode:\n",
        "    # -- Get parameters\n",
        "    alpha = alpha_fn(epi)\n",
        "    epsilon = epsilon_fn(epi)\n",
        "    # -- Run Game\n",
        "    game = Mock4()\n",
        "    a1 = ql.agent_policy(epsilon)\n",
        "    a2 = agent_greedy\n",
        "    result = game.play(a1, a2, p_msg=False, p_res=False)\n",
        "    reward = 1\n",
        "    if result == 0: # Draw\n",
        "      reward = 0\n",
        "      result = 1\n",
        "    # -- Append to Replay\n",
        "    S1_p, S1_o = None, None\n",
        "    while len(game.history) > 0:\n",
        "      a = int(game.undo() / game.h)\n",
        "      S0_p = game.tensor(player=result)\n",
        "      S0_o = game.tensor(player=(3 - result))\n",
        "      ql.replay.add(S0_p, a, reward, S1_p)\n",
        "      ql.replay.add(S0_o, a, -reward, S1_o)\n",
        "      if S1_p is None:\n",
        "        ql.replay_terminal.add(S0_p, a, reward, S1_p)\n",
        "        ql.replay_terminal.add(S0_o, a, -reward, S1_o)\n",
        "      S1_p, S1_o = S0_p, S0_o\n",
        "      reward = 0\n",
        "    # -- Sampling and learning\n",
        "    if len(ql.replay.b) >= sz_sample:\n",
        "      S_0, As, Rs, S_1 = ql.replay.sample(sz_sample)\n",
        "      # Append Terminal states\n",
        "      t_S_0, t_As, t_Rs, t_S_1 = ql.replay_terminal.sample(sz_sample_terminal)\n",
        "      S_0 += t_S_0\n",
        "      As += t_As\n",
        "      Rs += t_Rs\n",
        "      S_1  += t_S_1\n",
        "      # Tensor-fy\n",
        "      X_0 = torch.stack(S_0).to(device)\n",
        "      R = torch.tensor(Rs, dtype=torch.float).to(device)\n",
        "      Sz_1 = [torch.zeros(3, game.w, game.h) if s is None else s for s in S_1]\n",
        "      Snone_1 = [s is None for s in S_1]\n",
        "      X_1 = torch.stack(Sz_1).to(device)\n",
        "      # Calc Curr Q\n",
        "      with torch.no_grad():\n",
        "        Q_0 = ql.q(X_0)\n",
        "        Q_1 = ql.q(X_1)\n",
        "      Qa_0 = Q_0[range(len(As)), As]\n",
        "      Qmax_1 = torch.max(Q_1, dim=1).values\n",
        "      # Q_0 <- Q_0 + alpha * (R + gamma * max Q_1 - Q_0) if not terminated\n",
        "      Qtgt_0 = Qa_0 + alpha * (R + gamma * Qmax_1 - Qa_0)\n",
        "      # Q_0 <- R otherwise\n",
        "      Qtgt_0[Snone_1] = R[Snone_1]\n",
        "      # Clip\n",
        "      Qtgt_0[Qtgt_0 > 1.0] = 1.0\n",
        "      Qtgt_0[Qtgt_0 < -1.0] = -1.0\n",
        "      if init_left > 0:\n",
        "        init_left -= 1\n",
        "        Qtgt_0 = torch.zeros(Qtgt_0.shape).to(device)\n",
        "        print(\"INIT\")\n",
        "      # Learn\n",
        "      loss_list = []\n",
        "      for e in range(n_epoch):\n",
        "        opt.zero_grad()\n",
        "        Q_0 = ql.q(X_0)\n",
        "        Qa_0 = Q_0[range(len(As)), As]\n",
        "        loss = loss_fn(Qa_0, Qtgt_0)\n",
        "        loss_list.append(loss.mean().item())\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "      epi += 1\n",
        "      print(\"Ep #{} (#Repl={}) Loss {:.8f}α -> {:.8f}α\".format(\n",
        "          epi, len(ql.replay.b), loss_list[0] / alpha, loss_list[-1] / alpha))\n",
        "      # Update Policy\n",
        "      if (epi + 1) % int_policy_update == 0:\n",
        "        ql.update_policy(ql.policy, ql.q)\n",
        "    else: print(\"Accumulating Replay... (#={})\".format(len(ql.replay.b)))\n",
        "ql.learn = _learn"
      ],
      "metadata": {
        "id": "8toPGhWluLqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run():\n",
        "  ql.init_nn()\n",
        "  opt = optim.Adam(ql.q.parameters(), lr=1e-3, weight_decay=1e-6)\n",
        "  loss_fn = nn.SmoothL1Loss()\n",
        "  n_episode = 1000\n",
        "  n_epoch = 10\n",
        "  int_policy_update = 10\n",
        "  alpha_fn = lambda n: 1\n",
        "  gamma = 0.99\n",
        "  epsilon_fn = lambda n: max(0.05, 0.3 * (0.99 ** n))\n",
        "  sz_sample_terminal = 256\n",
        "  sz_sample = 4096 - sz_sample_terminal\n",
        "\n",
        "  ql.learn(\n",
        "      opt=opt,\n",
        "      loss_fn=loss_fn,\n",
        "      n_episode=n_episode,\n",
        "      n_epoch=n_epoch,\n",
        "      int_policy_update=int_policy_update,\n",
        "      alpha_fn=alpha_fn,\n",
        "      gamma=gamma,\n",
        "      epsilon_fn=epsilon_fn,\n",
        "      sz_sample=sz_sample,\n",
        "      sz_sample_terminal=sz_sample_terminal\n",
        "  )\n",
        "run()"
      ],
      "metadata": {
        "id": "ZTFnd-xWuNAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_mock4(100, agent_random, ql.agent_policy(0))\n",
        "test_mock4(100, agent_greedy, ql.agent_policy(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcOmJ0Lyu2kb",
        "outputId": "d3f1a78e-9a28-4a85-8d40-c7a2a3020447"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** Test\n",
            "* A1 = <function agent_random at 0x7fc6a3aaf0e0>\n",
            "* A2 = <function _agent_policy.<locals>.agent at 0x7fc5e0a5f0e0>\n",
            "Total = 100 games\n",
            "W1 48 (0.480) / Dr 0 (0.000) / W2 52 (0.520)\n",
            "** Test\n",
            "* A1 = <function agent_greedy at 0x7fc6a3aaf710>\n",
            "* A2 = <function _agent_policy.<locals>.agent at 0x7fc5e0a5fe60>\n",
            "Total = 100 games\n",
            "W1 100 (1.000) / Dr 0 (0.000) / W2 0 (0.000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습 방법\n",
        "\n",
        "- 학습은 처음에 약 $20000$개의 중간상태\n",
        "($< 5000$개의 기보)를 만들어둔 상태에서\n",
        "매 episode를 진행하면서 이전 replay를 가져와\n",
        "학습하도록 하였습니다.\n",
        "- 상대 agent로는 현재 이기기 위한 목표인\n",
        "`agent_greedy` (analysis-based)를 사용했습니다.\n",
        "- 한번에 $4096$개의 중간상태 ($\\simeq 100$개의 기보)\n",
        "를 batch로 가져와서 $q$를 근사하며,\n",
        "이를 1000번 반복합니다.\n",
        "\n",
        "### 결과\n",
        "\n",
        "- `agent_random` 상대로 약 50%의 승률을 얻고,\n",
        "`agent_greedy`에게는 완패합니다.\n",
        "- 학습이 진행되면 $q$가 특정 값으로 수렴해야 하는데,\n",
        "$\\alpha$를 어떻게 decay 하는지와 무관하게 잘 수렴하지\n",
        "않습니다.\n",
        "- 오목이나 사목 같은 경우에는 episode가 충분히 짧고\n",
        "중간 행동에 대한 reward가 없기 때문에 $q$를 이용해서\n",
        "근사하는 것보다는 Monte Carlo 방식으로 샘플을\n",
        "누적시키는 것이 훨씬 빠르게 수렴하는 것 같습니다.\n",
        "아마 위 방식도 훨씬 긴 시간동안 실행하면 수렴할 수도\n",
        "있을 것 같지만, 위 학습을 실행하는데 이미\n",
        "30분가량 소모되기 때문에 중단하였습니다."
      ],
      "metadata": {
        "id": "bqauqI15zdcG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Monte Carlo Policy Gradient\n",
        "\n",
        "REINFORCE 알고리즘을 사용합니다.\n",
        "\n",
        "- Policy $\\pi$에 대한 neural network를 구성합니다.\n",
        "network structure는 위 DQN에서 사용한 $q$와\n",
        "완전히 동일하되, batch normalization만 제외하며,\n",
        "출력에 softmax를 사용합니다.\n",
        "- Policy가 indicator function이 아니기 때문에\n",
        "행동은 확률적으로 시행됩니다.\n",
        "애초에 $q$가 없기도 하고, 확률적으로 시행하기 때문에\n",
        "자동으로 탐색을 하므로, $\\epsilon$ 같은\n",
        "매개변수는 없습니다.\n",
        "- Gradient ascent에 torch의 optimizer를 사용합니다.\n",
        "- Reward는 게임이 끝난 시점에서만 주어지므로,\n",
        "return $G$는 게임 결과의 상수배로 나타납니다.\n",
        "기존에는 이 $G$가 이겼으면 $G_t = \\gamma^{T-t} > 0$,\n",
        "지면 $G_t = - \\gamma^{T-t} < 0$, 무승부면 $G = 0$으로 주었는데, \n",
        "여기서는 $- G \\log \\pi$, 즉 $\\pi$의 negative loss\n",
        "에 return을 곱한 것을 loss로 사용하기 때문에,\n",
        "return $G$가 음수라면 weight exploding이\n",
        "발생합니다.\n",
        "따라서, 승패 보상을 $[0, 1]$로 조절합니다.\n",
        "- 추가적으로, 여기서는 NLL Loss 같은 loss function\n",
        "을 사용하는데, NLL Loss는 label\n",
        "(여기서는 return $G$)가 0이면 loss가 0이기 때문에\n",
        "학습에 영향을 끼치지 않습니다.\n",
        "즉, 이런 경우인 패배하는 경우는 여기서는 학습 데이터로\n",
        "사용하지 않습니다.\n"
      ],
      "metadata": {
        "id": "1STVPHqNu6an"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mcpg = Scope()"
      ],
      "metadata": {
        "id": "51XDAjjiHseV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## nn\n",
        "def _new_nn():\n",
        "  W = 7\n",
        "  H = 6\n",
        "  net = nn.Sequential(\n",
        "      # 01\n",
        "      nn.Conv2d(3, 64, 3, padding='same'),\n",
        "      nn.ReLU(),\n",
        "      # 02\n",
        "      nn.Conv2d(64, 64, 3, padding='same'),\n",
        "      nn.MaxPool2d(2),\n",
        "      nn.ReLU(),\n",
        "      # 03\n",
        "      nn.Conv2d(64, 32, 3, padding='same'),\n",
        "      nn.ReLU(),\n",
        "      # 04\n",
        "      nn.Conv2d(32, 32, 3, padding='same'),\n",
        "      nn.ReLU(),\n",
        "      # 05\n",
        "      nn.Conv2d(32, 8, 3, padding='same'),\n",
        "      nn.ReLU(),\n",
        "      # Lin01\n",
        "      Flatten(),\n",
        "      nn.Linear(8 * (W // 2) * (H // 2), 20),\n",
        "      nn.ReLU(),\n",
        "      # Lin02\n",
        "      nn.Linear(20, W),\n",
        "      # Softmax\n",
        "      nn.Softmax(dim=-1)\n",
        "  ).to(device)\n",
        "  return net\n",
        "mcpg.new_nn = _new_nn\n",
        "\n",
        "def _init_nn():\n",
        "  mcpg.policy = mcpg.new_nn()\n",
        "mcpg.init_nn = _init_nn"
      ],
      "metadata": {
        "id": "1s8p794PH6ni"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _argrnd(tensor):\n",
        "  # Choosed argument randomly by p, the probablity of each arguments\n",
        "  # e.g. if p=[0.2, 0.5, 0.3], choose 0 in 20%, choose 1 in 50%, choose 2 in 30%\n",
        "  if len(tensor.shape) == 1:\n",
        "    p = tensor.to('cpu').numpy()\n",
        "    p = p / p.sum()\n",
        "    return torch.tensor(np.random.choice(tensor.shape[0], p=p))\n",
        "  elif len(tensor.shape) == 2:\n",
        "    a = []\n",
        "    for i in range(tensor.shape[0]):\n",
        "      p = tensor[i, :].to('cpu').numpy()\n",
        "      p = p / p.sum()\n",
        "      a.append(np.random.choice(tensor.shape[1], p=p))\n",
        "    return torch.tensor(a)\n",
        "  else: raise ValueError(\"Unsupported Shape {} for argrnd\".format(tensor.shape))\n",
        "mcpg.argrnd = _argrnd"
      ],
      "metadata": {
        "id": "dH1Nd2zoPEnl"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Policy = epsilon-greedy for policy\n",
        "def _agent_policy(epsilon):\n",
        "  def agent(game):\n",
        "    if np.random.uniform() < epsilon: return agent_random(game)\n",
        "    X = game.tensor().unsqueeze(dim=0).to(device)\n",
        "    M = game.tensor_full()\n",
        "    with torch.no_grad():\n",
        "      Q = mcpg.policy(X)\n",
        "      Q = Q.squeeze(dim=0)\n",
        "    Q[M] = 0.0\n",
        "    A = mcpg.argrnd(Q).item()\n",
        "    return A\n",
        "  return agent\n",
        "mcpg.agent_policy = _agent_policy"
      ],
      "metadata": {
        "id": "-LfAxvisH6qU"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _loss_fn(policy, v):\n",
        "  # Original GD (minimization) : theta <- theta - alpha D J(theta)\n",
        "  # In policy gradient (maximization) : theta <- theta + alpha v D log pi\n",
        "  # => Negative log likelihood weighted by reward v\n",
        "  return - (v * torch.log(policy)).mean()\n",
        "mcpg.loss_fn = _loss_fn"
      ],
      "metadata": {
        "id": "pUxyvVbvI0MR"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# REINFORCE\n",
        "def _learn(\n",
        "    opt,\n",
        "    n_episode,\n",
        "    n_epoch,\n",
        "    gamma,\n",
        "    batch_size\n",
        "):\n",
        "  epi = 0\n",
        "  Xs, As, Vs = [], [], []\n",
        "  while epi < n_episode:\n",
        "    # -- Run Game\n",
        "    game = Mock4()\n",
        "    a1 = mcpg.agent_policy(0)\n",
        "    a2 = agent_greedy\n",
        "    result = game.play(a1, a2, p_msg=False, p_res=False)\n",
        "    reward = 1\n",
        "    if result == 0: # Draw\n",
        "      reward = 0.5\n",
        "      result = 1\n",
        "    # -- Append to Replay\n",
        "    while len(game.history) > 0:\n",
        "      a = int(game.undo() / game.h)\n",
        "      S0_p = game.tensor(player=result)\n",
        "      Xs.append(S0_p)\n",
        "      As.append(a)\n",
        "      Vs.append(reward)\n",
        "      reward *= gamma\n",
        "    if len(Xs) >= batch_size:\n",
        "      # Tensor-fy\n",
        "      X = torch.stack(Xs).to(device)\n",
        "      A = torch.tensor(As).unsqueeze(dim=1).to(device)\n",
        "      V = torch.tensor(Vs, dtype=torch.float).to(device)\n",
        "      # Learn\n",
        "      loss_list = []\n",
        "      for e in range(n_epoch):\n",
        "        opt.zero_grad()\n",
        "        pi_s = mcpg.policy(X)\n",
        "        pi_sa = pi_s.gather(1, A).squeeze(dim=1)\n",
        "        loss = mcpg.loss_fn(pi_sa, V)\n",
        "        loss_list.append(loss.mean().item())\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "      if epi % 100 == 0:\n",
        "        print(\"Ep #{:<6d} Loss {:13.10f} -> {:13.10f}\".format(\n",
        "          epi, loss_list[0], loss_list[-1]))\n",
        "      Xs, As, Vs = [], [], []\n",
        "      epi += 1\n",
        "mcpg.learn = _learn"
      ],
      "metadata": {
        "id": "OgIgpwq-H6tt"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run():\n",
        "  mcpg.init_nn()\n",
        "\n",
        "  opt = optim.Adam(mcpg.policy.parameters(), lr=1e-2, weight_decay=1e-5)\n",
        "  n_episode = 20000\n",
        "  n_epoch = 2\n",
        "  gamma = 0.99\n",
        "  batch_size = 50\n",
        "\n",
        "  mcpg.learn(\n",
        "      opt=opt,\n",
        "      n_episode=n_episode,\n",
        "      n_epoch=n_epoch,\n",
        "      gamma=gamma,\n",
        "      batch_size=batch_size\n",
        "  )\n",
        "run()"
      ],
      "metadata": {
        "id": "uY1mKYW1KiI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_mock4(1000, agent_random, mcpg.agent_policy(0))\n",
        "test_mock4(1000, agent_greedy, mcpg.agent_policy(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFIkBVZHWJ1H",
        "outputId": "9be9db10-c9fe-48a6-a95d-a5d9f6147e4a"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** Test\n",
            "* A1 = <function agent_random at 0x7f9ce4cfb0e0>\n",
            "* A2 = <function _agent_policy.<locals>.agent at 0x7f9c08c4a5f0>\n",
            "Total = 1000 games\n",
            "W1 302 (0.302) / Dr 1 (0.001) / W2 697 (0.697)\n",
            "** Test\n",
            "* A1 = <function agent_greedy at 0x7f9ce4cfb710>\n",
            "* A2 = <function _agent_policy.<locals>.agent at 0x7f9c08c4a050>\n",
            "Total = 1000 games\n",
            "W1 995 (0.995) / Dr 0 (0.000) / W2 5 (0.005)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습방법\n",
        "\n",
        "- 원래 알고리즘이 각 episode 별로 gradient ascent\n",
        "를 시행하기 때문에, 여기서 batch는\n",
        "약 2개의 episode가 들어갈 정도인 50을 사용합니다.\n",
        "- 하나의 batch에 대해 gradient ascent는 2번만\n",
        "합니다.\n",
        "(원래 REINFORCE는 하나의 episode에 대해 한번씩만\n",
        "ascent를 하는데, loss의 변화를 보기 위해 2번 합니다.)\n",
        "- Episode의 수는 학습시간이 위의 DQN의\n",
        "학습시간인 30분과 비슷하게 맞추기\n",
        "위해 $40000$\n",
        "(size 50의 batch로 20000번 학습하는데,\n",
        "batch당 약 2개의 episode가 들어가므로)\n",
        "개의 게임을 진행합니다.\n",
        "- 상대 agent로는 `agent_greedy`를 사용합니다."
      ],
      "metadata": {
        "id": "p4AYgc2h5-eq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 결과\n",
        "\n",
        "- `agent_random`을 상대로는 약 70%,\n",
        "`agent_greedy`를 상대로는 약 0.5%의 승률을 보입니다.\n",
        "- Policy가 확률적으로 동작하기 때문에, 실행을 할 때마다\n",
        "결과는 달라지며, Softmax function with temperature\n",
        "$S(\\textbf{x}; \\tau) = \\text{Softmax}(\\tau^{-1}\\textbf{x})$를 사용하면\n",
        "$\\pi$에서 낮은 확률을 갖는 경우를 더 쳐내고, 높은\n",
        "확률을 가지는 경우만을 골라내는 것도 가능합니다.\n",
        "- 지금까지의 시도 중 처음으로 `agent_greedy`를\n",
        "이기는 경우가 보입니다.\n",
        "아직 승률이 매우 낮지만,\n",
        "어쨌든 Connect4도 최대 7가지의 경우의 수를 최소\n",
        "4번을 적절히 골라야 이길 수 있기 때문에,\n",
        "$\\pi$가 최선/차선의 선택 정도는 학습했다고 봐도\n",
        "괜찮을 것 같습니다."
      ],
      "metadata": {
        "id": "YPKCQ4JmvdVx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 결론\n",
        "\n",
        "- 지금까지의 시도들 중 policy graident는 처음으로,\n",
        "그리고 유일하게 analysis-based greedy algorithm을\n",
        "상대로 승리를 보여주었습니다.\n",
        "- Policy gradient로 이길 수 있었던 것이 위 DQN에서는\n",
        "$\\epsilon$-greedy로 탐색을 하는데 반해\n",
        "policy gradient는 $\\pi$ 자체를 일종의 확률 분포로\n",
        "탐색을 하다보니 이기는 행동을 더 빠르게 탐색할 수 있지\n",
        "않았나 싶습니다.\n",
        "$q$에도 $\\epsilon$-greedy가 아니라 softmax를 하여\n",
        "policy를 추출하는 것은 가능한데, 이 경우에도\n",
        "비슷한 결과를 얻을 수 있을지는 의문입니다.\n",
        "(다만, DQN 방식대로 replay memory로 학습하는\n",
        "것은 현재로서는 너무 오래 걸리다보니\n",
        "나중에 시험해보기로 합니다.)\n",
        "- 이제 policy gradient를 통해\n",
        "승리할 수 있는 수가 $\\pi$에서 비교적 높은 확률로\n",
        "나타나므로, 이를 이용해서 tree search를 해보도록\n",
        "합니다."
      ],
      "metadata": {
        "id": "whFc9ouc-dYW"
      }
    }
  ]
}